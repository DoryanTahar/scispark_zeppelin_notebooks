{
  "paragraphs": [
    {
      "text": "%md\n\u003ch1\u003eParallel Statistical Rollups for a Time-Series of Grids\u003c/h1\u003e\n\n\u003ch3\u003eGeneral Problem:\u003c/h3\u003e\n\u003cb\u003eCompute per-pixel statistics for a physical variable over time for a global or regional lat/lon grid.\u003c/b\u003e\n\u003ch3\u003eSteps:\u003c/h3\u003e\nGiven daily files or OpeNDAP URL\u0027s over 10 - 30 years, load the grids and mask missing values.\nThen compute statistics of choice:  count, mean, standard deviation, minimum, maximum, skewness, kurtosis, etc.\nOptionally \"roll up\" the statistics over time by sub-periods of choice:  month, season, year, 5-year, and total period.\n\u003cbr\u003e\n",
      "dateUpdated": "Jul 18, 2016 8:49:53 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467954681495_603120411",
      "id": "20160707-221121_938481638",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eParallel Statistical Rollups for a Time-Series of Grids\u003c/h1\u003e\n\u003ch3\u003eGeneral Problem:\u003c/h3\u003e\n\u003cp\u003e\u003cb\u003eCompute per-pixel statistics for a physical variable over time for a global or regional lat/lon grid.\u003c/b\u003e\u003c/p\u003e\n\u003ch3\u003eSteps:\u003c/h3\u003e\n\u003cp\u003eGiven daily files or OpeNDAP URL\u0027s over 10 - 30 years, load the grids and mask missing values.\n\u003cbr  /\u003eThen compute statistics of choice:  count, mean, standard deviation, minimum, maximum, skewness, kurtosis, etc.\n\u003cbr  /\u003eOptionally \u0026ldquo;roll up\u0026rdquo; the statistics over time by sub-periods of choice:  month, season, year, 5-year, and total period.\n\u003cbr  /\u003e\u003cbr\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 7, 2016 10:11:21 PM",
      "dateStarted": "Jul 18, 2016 8:49:54 PM",
      "dateFinished": "Jul 18, 2016 8:49:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003ch3\u003eExample Application:\u003c/h3\u003e\nDataset is daily TRMM precipitation (in mm) over period 1998 - 2012.\nA total of 5,475 netCDF files.\nVariable name is \"r\".\nLat/lon resolution is approx. quarter degree, 1440 x 840.\n\u003cbr\u003e\n",
      "dateUpdated": "Jul 18, 2016 5:29:12 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "tableHide": false,
        "editorHide": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468122955707_818154629",
      "id": "20160709-205555_892715033",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eExample Application:\u003c/h3\u003e\n\u003cp\u003eDataset is daily TRMM precipitation (in mm) over period 1998 - 2012.\n\u003cbr  /\u003eA total of 5,475 netCDF files.\n\u003cbr  /\u003eVariable name is \u0026ldquo;r\u0026rdquo;.\n\u003cbr  /\u003eLat/lon resolution is approx. quarter degree, 1440 x 840.\n\u003cbr  /\u003e\u003cbr\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 9, 2016 8:55:55 PM",
      "dateStarted": "Jul 18, 2016 5:29:12 PM",
      "dateFinished": "Jul 18, 2016 5:29:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Rollup Statistics By Month, season, Year, 5-Year, and Total",
      "text": "%md\n ![Statistical Rollups](https://raw.githubusercontent.com/SciSpark/scispark_zeppelin_notebooks/master/images/Rollup.png)",
      "dateUpdated": "Jul 18, 2016 8:42:20 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468880485848_2097289555",
      "id": "20160718-152125_177418516",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003cimg src\u003d\"https://raw.githubusercontent.com/SciSpark/scispark_zeppelin_notebooks/master/images/Rollup.png\" alt\u003d\"Statistical Rollups\" /\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 18, 2016 3:21:25 PM",
      "dateStarted": "Jul 18, 2016 8:42:21 PM",
      "dateFinished": "Jul 18, 2016 8:42:21 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\nls -d -1 /home/workshop/TRMM_daily/*.* \u003e\u003e /home/workshop/urls_trmm_daily_2012.txt",
      "dateUpdated": "Jul 18, 2016 8:41:23 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468896226015_-1422039135",
      "id": "20160718-194346_637011908",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Process exited with an error: 1 (Exit value: 1)"
      },
      "dateCreated": "Jul 18, 2016 7:43:46 PM",
      "dateStarted": "Jul 18, 2016 8:41:23 PM",
      "dateFinished": "Jul 18, 2016 8:44:24 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Driver File contains time-Sorted List of URL\u0027s or Local Paths (using OPeNDAP to pull variables would Lengthen I/O time)",
      "text": "%pyspark\nvariable \u003d \u0027r\u0027\n#urlListFile \u003d \"/data/cluster-local/bdwilson/Climatology/clim/urls_trmm_daily_1998_2012.txt\"\nurlListFile \u003d \"/data/cluster-local/bdwilson/Climatology/clim/urls_trmm_daily_2012.txt\"\n#urlListFile \u003d \"/home/workshop/urls_trmm_daily_1998_2012.txt\"\nwith open(urlListFile, \u0027r\u0027) as f:\n    urlList \u003d f.readlines()\nprint \u0027\u0027.join(urlList[:20])",
      "dateUpdated": "Jul 18, 2016 7:48:12 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true,
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468124355434_-2138924431",
      "id": "20160709-211915_1244001387",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.01.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.02.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.03.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.04.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.05.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.06.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.07.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.08.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.09.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.10.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.11.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.12.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.13.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.14.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.15.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.16.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.17.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.18.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.19.7.nc\n/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.20.7.nc\n\n"
      },
      "dateCreated": "Jul 9, 2016 9:19:15 PM",
      "dateStarted": "Jul 18, 2016 7:48:12 PM",
      "dateFinished": "Jul 18, 2016 7:48:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Import functions from Clim Library",
      "text": "%pyspark\nimport sys, os, re, time\nimport numpy as N\nfrom netCDF4 import Dataset, default_fillvals\n\nfrom clim.variables import getVariables, close\nfrom clim.split import splitByMonth\nfrom clim.cache import retrieveFile",
      "dateUpdated": "Jul 18, 2016 7:48:18 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468125012089_-359515487",
      "id": "20160709-213012_1829572151",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 9, 2016 9:30:12 PM",
      "dateStarted": "Jul 18, 2016 7:48:18 PM",
      "dateFinished": "Jul 18, 2016 7:48:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Split URL\u0027s by Month -- Special split to support monthly, Seasonal, Yearly, ... and Total stats",
      "text": "%pyspark\nurlsByKey \u003d splitByMonth(urlList, {\u0027get\u0027: (\u0027year\u0027, \u0027month\u0027), \u0027regex\u0027: re.compile(r\u0027\\/3B42_daily.(....).(..)\u0027)})\nprint urlsByKey[0]",
      "dateUpdated": "Jul 18, 2016 7:48:22 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468123077611_-927314857",
      "id": "20160709-205757_1988707763",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "((\u00272012\u0027, \u002701\u0027), [\u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.01.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.02.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.03.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.04.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.05.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.06.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.07.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.08.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.09.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.10.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.11.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.12.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.13.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.14.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.15.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.16.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.17.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.18.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.19.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.20.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.21.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.22.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.23.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.24.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.25.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.26.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.27.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.28.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.29.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.30.7.nc\u0027, \u0027/storage/users/obs4Mips/TRMM_daily/3B42_daily.2012.01.31.7.nc\u0027])\n"
      },
      "dateCreated": "Jul 9, 2016 8:57:57 PM",
      "dateStarted": "Jul 18, 2016 7:48:22 PM",
      "dateFinished": "Jul 18, 2016 7:48:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Accumulate function -- Update accumulators for a set of Variable grids",
      "text": "%pyspark\ndef accumulate(urls, variable, accumulators\u003d[\u0027count\u0027, \u0027mean\u0027, \u0027M2\u0027, \u0027min\u0027, \u0027max\u0027], cachePath\u003d\u0027~/cache\u0027):\n    \u0027\u0027\u0027Accumulate data into statistics accumulators like count, sum, sumsq, min, max, M3, M4, etc.\u0027\u0027\u0027\n    keys, urls \u003d urls\n    accum \u003d {}\n    for i, url in enumerate(urls):\n        try:\n            path \u003d retrieveFile(url, cachePath)\n        except:\n            print \u003e\u003esys.stderr, \u0027accumulate: Error, continuing without file %s\u0027 % url\n            continue\n\n        try:\n            print \u003e\u003esys.stderr, \u0027Reading %s ...\u0027 % path\n            var, fh \u003d getVariables(path, [variable], arrayOnly\u003dTrue, set_auto_mask\u003dTrue)   # return dict of variable objects by name                                                \n            v \u003d var[variable]   # could be masked array                                                                                                                             \n            if v.shape[0] \u003d\u003d 1: v \u003d v[0]    # throw away trivial time dimension for CF-style files                                                                                  \n            close(fh)\n        except:\n            print \u003e\u003esys.stderr, \u0027accumulate: Error, cannot read variable %s from file %s\u0027 % (variable, path)\n            continue\n\n        if i \u003d\u003d 0:\n            for k in accumulators:\n                if k \u003d\u003d \u0027min\u0027:     accum[k] \u003d default_fillvals[\u0027f8\u0027] * N.ones(v.shape, dtype\u003dN.float64)\n                elif k \u003d\u003d \u0027max\u0027:   accum[k] \u003d -default_fillvals[\u0027f8\u0027] * N.ones(v.shape, dtype\u003dN.float64)\n                elif k \u003d\u003d \u0027count\u0027: accum[k] \u003d N.zeros(v.shape, dtype\u003dN.int64)\n                else:\n                    accum[k] \u003d N.zeros(v.shape, dtype\u003dN.float64)\n\n        if N.ma.isMaskedArray(v):\n            if \u0027count\u0027 in accumulators:\n                accum[\u0027count\u0027] +\u003d ~v.mask\n            if \u0027min\u0027 in accumulators:\n                accum[\u0027min\u0027] \u003d N.ma.minimum(accum[\u0027min\u0027], v)\n            if \u0027max\u0027 in accumulators:\n                accum[\u0027max\u0027] \u003d N.ma.maximum(accum[\u0027max\u0027], v)\n\n            v \u003d N.ma.filled(v, 0.)\n    \telse:\n            if \u0027count\u0027 in accumulators:\n                accum[\u0027count\u0027] +\u003d 1\n            if \u0027min\u0027 in accumulators:\n                accum[\u0027min\u0027] \u003d N.minimum(accum[\u0027min\u0027], v)\n            if \u0027max\u0027 in accumulators:\n                accum[\u0027max\u0027] \u003d N.maximum(accum[\u0027max\u0027], v)\n\n        if \u0027mean\u0027 in accumulators:\n            n \u003d accum[\u0027count\u0027]\n            delta \u003d v - accum[\u0027mean\u0027]     # subtract running mean from new values, eliminate roundoff errors\n            delta_n \u003d delta / n\n            accum[\u0027mean\u0027] +\u003d delta_n\n        if \u0027M2\u0027 in accumulators:\n            term \u003d delta * delta_n * (n-1)    \n            accum[\u0027M2\u0027] +\u003d term\n    return (keys, accum)",
      "dateUpdated": "Jul 18, 2016 8:19:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468125433968_498455947",
      "id": "20160709-213713_495638626",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 9, 2016 9:37:13 PM",
      "dateStarted": "Jul 18, 2016 7:48:26 PM",
      "dateFinished": "Jul 18, 2016 7:48:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Combine Function -- Merge Accumulators to go from monthly to seasonal to Yearly to Total",
      "text": "%pyspark\ndef combine(a, b, accumulators\u003d[\u0027count\u0027, \u0027mean\u0027, \u0027M2\u0027, \u0027min\u0027, \u0027max\u0027]):\n    \u0027\u0027\u0027Combine accumulators by summing.\u0027\u0027\u0027\n    keys, a \u003d a\n    b \u003d b[1]\n    if \u0027mean\u0027 in accumulators:\n        ntotal \u003d a[\u0027count\u0027] + b[\u0027count\u0027]\n        a[\u0027mean\u0027] \u003d (a[\u0027mean\u0027] * a[\u0027count\u0027] + b[\u0027mean\u0027] * b[\u0027count\u0027]) / ntotal\n    if \u0027min\u0027 in accumulators:\n        if N.ma.isMaskedArray(a):\n            a[\u0027min\u0027] \u003d N.ma.minimum(a[\u0027min\u0027], b[\u0027min\u0027])\n        else:\n            a[\u0027min\u0027] \u003d N.minimum(a[\u0027min\u0027], b[\u0027min\u0027])\n    if \u0027max\u0027 in accumulators:\n        if N.ma.isMaskedArray(a):\n            a[\u0027max\u0027] \u003d N.ma.maximum(a[\u0027max\u0027], b[\u0027max\u0027])\n        else:\n            a[\u0027max\u0027] \u003d N.maximum(a[\u0027max\u0027], b[\u0027max\u0027])\n    for k in a.keys():\n        if k !\u003d \u0027mean\u0027 and k !\u003d \u0027min\u0027 and k !\u003d \u0027max\u0027:\n            a[k] +\u003d b[k]      # just sum count and other moments\n    return ((\u0027total\u0027,), a)",
      "dateUpdated": "Jul 18, 2016 7:48:33 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468125745831_-253482966",
      "id": "20160709-214225_1098618592",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 9, 2016 9:42:25 PM",
      "dateStarted": "Jul 18, 2016 7:48:30 PM",
      "dateFinished": "Jul 18, 2016 7:48:31 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define function to compute final statistics from the Accumulators",
      "text": "%pyspark\ndef statsFromAccumulators(accum):\n    \u0027\u0027\u0027Compute final statistics from accumulators.\u0027\u0027\u0027\n    keys, accum \u003d accum\n    \n    # Mask all of the accumulator arrays                                                                                                                                            \n    accum[\u0027count\u0027] \u003d N.ma.masked_equal(accum[\u0027count\u0027], 0, copy\u003dFalse)\n    mask \u003d accum[\u0027count\u0027].mask\n    for k in accum:\n        if k !\u003d \u0027count\u0027:\n\t    accum[k] \u003d N.ma.array(accum[k], copy\u003dFalse, mask\u003dmask)\n\n    # Compute stats (masked)                                                                                                                                                       \n    stats \u003d {}\n    if \u0027count\u0027 in accum:\n        stats[\u0027count\u0027] \u003d accum[\u0027count\u0027]\n    if \u0027min\u0027 in accum:\n        stats[\u0027min\u0027] \u003d accum[\u0027min\u0027]\n    if \u0027max\u0027 in accum:\n        stats[\u0027max\u0027] \u003d accum[\u0027max\u0027]\n    if \u0027mean\u0027 in accum:\n        stats[\u0027mean\u0027] \u003d accum[\u0027mean\u0027]\n    if \u0027M2\u0027 in accum:\n        stats[\u0027stddev\u0027] \u003d N.sqrt(accum[\u0027M2\u0027] / (accum[\u0027count\u0027] - 1))\n    return (keys, stats)",
      "dateUpdated": "Jul 18, 2016 7:48:39 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468125898517_163062175",
      "id": "20160709-214458_1383645253",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 9, 2016 9:44:58 PM",
      "dateStarted": "Jul 18, 2016 7:48:39 PM",
      "dateFinished": "Jul 18, 2016 7:48:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Define Function to write Stats to netCDF file with some attributes",
      "text": "%pyspark\ndef writeStats(inputFile, variable, stats, outFile, coordinates\u003d[\u0027latitude\u0027, \u0027longitude\u0027],\n               copyToHdfsPath\u003dNone, format\u003d\u0027NETCDF4\u0027, cachePath\u003d\u0027~/cache\u0027):\n    \u0027\u0027\u0027Write out stats arrays to netCDF with some attributes.                                                                                                                       \n    \u0027\u0027\u0027\n    keys, stats \u003d stats\n    if os.path.exists(outFile): os.unlink(outFile)\n    dout \u003d Dataset(outFile, \u0027w\u0027, format\u003dformat)\n    print \u003e\u003esys.stderr, \u0027Writing %s ...\u0027 % outFile\n    dout.setncattr(\u0027variable\u0027, variable)\n    dout.setncattr(\u0027urls\u0027, str(urlList))\n    dout.setncattr(\u0027level\u0027, str(keys))\n\n#    inFile \u003d retrieveFile(inputFile, cachePath)\n    print \u003e\u003esys.stderr, \u0027Reading coordinates \u0026 attributes from %s\u0027 % inFile\n    din \u003d Dataset(inputFile, \u0027r\u0027)\n    try:\n        coordinatesFromFile \u003d din.variables[variable].getncattr(\u0027coordinates\u0027)\n        coordinates \u003d coordinatesFromFile.split()\n        if coordinates[0] \u003d\u003d \u0027time\u0027:   coordinates \u003d coordinates[1:]    # discard trivial time dimension for CF-style files                                                         \n    except:\n        if coordinates is None or len(coordinates) \u003d\u003d 0:\n            coordinates \u003d (\u0027lat\u0027, \u0027lon\u0027)     # kludge: another possibility \n\n    # Add dimensions and variables, copying data                                                                                                                                    \n    coordDim \u003d [dout.createDimension(coord, din.variables[coord].shape[0]) for coord in coordinates]     # here lat, lon, alt, etc.                                                 \n    for coord in coordinates:\n        var \u003d dout.createVariable(coord, din.variables[coord].dtype, (coord,))\n        var[:] \u003d din.variables[coord][:]\n\n    # Add stats variables                                                                                                                                                           \n    for k,v in stats.items():\n        var \u003d dout.createVariable(k, stats[k].dtype, coordinates)\n        var[:] \u003d v[:]\n\n    din.close()\n    dout.close()\n    return outFile",
      "dateUpdated": "Jul 18, 2016 7:55:04 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468125921227_535899184",
      "id": "20160709-214521_472574405",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 9, 2016 9:45:21 PM",
      "dateStarted": "Jul 18, 2016 7:48:48 PM",
      "dateFinished": "Jul 18, 2016 7:48:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Plot Routines",
      "text": "%pyspark\nimport matplotlib as mpl\nmpl.use(\u0027Agg\u0027)\nimport matplotlib.pyplot as plt\nimport cStringIO\nimport base64\nfrom mpl_toolkits.basemap import Basemap\nfrom clim.plotlib import imageMap\n\ndef show(p):\n    img \u003d cStringIO.StringIO()\n    p.savefig(img, format\u003d\u0027png\u0027)\n    imgStr \u003d \"data:image/png;base64,\"\n    imgStr +\u003d base64.b64encode(img.getvalue().strip())\n    print \"%html \u003cimg src\u003d\u0027\" + imgStr + \"\u0027\u003e\" \n    \ndef plotStats(inputFile, stat, metric\u003d\u0027mean\u0027, coordinates\u003d[\u0027latitude\u0027, \u0027longitude\u0027]):\n    \u0027\u0027\u0027Plot per-pixel statistics on a map.\u0027\u0027\u0027\n    keys, stats \u003d stats\n    if os.path.exists(outFile): os.unlink(outFile)\n    coords, fh \u003d getVariables(inputFile, coordinates, arrayOnly\u003dTrue)\n    plt \u003d imageMap(coords[\u0027longitude\u0027], coords[\u0027latitude\u0027], stat[\u0027metric\u0027])\n    show(plt)\n    \n#plotStats(inputFile, stat, \u0027mean\u0027)",
      "dateUpdated": "Jul 18, 2016 8:50:49 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468896904265_-1540380332",
      "id": "20160718-195504_509088452",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 225, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 21, in \u003cmodule\u003e\nNameError: name \u0027inputFile\u0027 is not defined\n"
      },
      "dateCreated": "Jul 18, 2016 7:55:04 PM",
      "dateStarted": "Jul 18, 2016 8:48:21 PM",
      "dateFinished": "Jul 18, 2016 8:48:52 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Now to use sciSpark",
      "text": "%md\nThe Notebook automatically gives you a predefined SparkContext object as \"sc\".\n\nIf running from the command line, add code like:\n    from pyspark import SparkContext                                                                                                                                                \n    sc \u003d SparkContext(appName\u003d\"PixelStats\")\n    \nAnd then run a command line like:\n    spark-submit --master $MESOS_MASTER --total-executor-cores 16  pixelStats.py urls_trmm_daily_1998_2012.txt r trmm_precip_stats_1998_2012.nc",
      "dateUpdated": "Jul 18, 2016 7:49:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468126707877_-1533543677",
      "id": "20160709-215827_1479642374",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThe Notebook automatically gives you a predefined SparkContext object as \u0026ldquo;sc\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003eIf running from the command line, add code like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efrom pyspark import SparkContext                                                                                                                                                \nsc \u003d SparkContext(appName\u003d\"PixelStats\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd then run a command line like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003espark-submit --master $MESOS_MASTER --total-executor-cores 16  pixelStats.py urls_trmm_daily_1998_2012.txt r trmm_precip_stats_1998_2012.nc\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Jul 9, 2016 9:58:27 PM",
      "dateStarted": "Jul 18, 2016 7:49:25 PM",
      "dateFinished": "Jul 18, 2016 7:49:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Split the months across the cluster (just URL lists, data is read Later on-the-fly)",
      "text": "%pyspark\nurlsRDD \u003d sc.parallelize(urlsByKey, numSlices\u003d16)        # returns partitioned RDD of URL lists",
      "dateUpdated": "Jul 18, 2016 7:49:31 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468125928640_1780192451",
      "id": "20160709-214528_1372681724",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 9, 2016 9:45:28 PM",
      "dateStarted": "Jul 18, 2016 7:49:31 PM",
      "dateFinished": "Jul 18, 2016 7:49:31 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Transformations are lazy, only an action triggers computation",
      "text": "%pyspark\ntmp \u003d urlsRDD.collect()      # RDD is lazy, so must collect to print something\nprint len(tmp)\nprint tmp[0]",
      "dateUpdated": "Jul 18, 2016 7:51:20 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468129682491_-1993679291",
      "id": "20160709-224802_252484464",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 2 cancelled part of cancelled job group zeppelin-20160709-224802_252484464\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n\u0027, JavaObject id\u003do143), \u003ctraceback object at 0x7f235783f6c8\u003e)"
      },
      "dateCreated": "Jul 9, 2016 10:48:02 PM",
      "dateStarted": "Jul 18, 2016 7:51:20 PM",
      "dateFinished": "Jul 18, 2016 8:41:58 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Map Accumulate Function across the months and return RDD of stats. Accumulators",
      "text": "%pyspark\naccumRDD \u003d urlsRDD.map(lambda urls: accumulate(urls, variable))    # returns RDD of stats accumulators                                                                         ",
      "dateUpdated": "Jul 18, 2016 5:29:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468126681156_491728560",
      "id": "20160709-215801_1650881600",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 9, 2016 9:58:01 PM",
      "dateStarted": "Jul 18, 2016 5:29:13 PM",
      "dateFinished": "Jul 18, 2016 5:29:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Reduce the accumulators using the combine function, returning merged accumulators (reduce is an action)",
      "text": "%pyspark\nmerged \u003d accumRDD.reduce(combine)                      # merge accumulators on head node\nprint merged[0]\naccumulators \u003d merged[1]\nprint accumulators.keys()\nprint accumulators[\u0027count\u0027]",
      "dateUpdated": "Jul 18, 2016 5:29:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468126823885_-801230247",
      "id": "20160709-220023_142126633",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "(\u0027total\u0027,)\n[\u0027count\u0027, \u0027max\u0027, \u0027min\u0027, \u0027mean\u0027, \u0027M2\u0027]\n[[5475 5475 5475 ..., 5475 5475 5475]\n [5475 5475 5475 ..., 5475 5475 5475]\n [5475 5475 5475 ..., 5475 5475 5475]\n ..., \n [5475 5475 5475 ..., 5475 5475 5475]\n [5475 5475 5475 ..., 5475 5475 5475]\n [5475 5475 5475 ..., 5475 5475 5475]]\n"
      },
      "dateCreated": "Jul 9, 2016 10:00:23 PM",
      "dateStarted": "Jul 18, 2016 5:29:13 PM",
      "dateFinished": "Jul 18, 2016 5:29:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Compute the overall statistics from the merged accumulators",
      "text": "%pyspark\nstats \u003d statsFromAccumulators(merged)     # compute total stats from merged accumulators\n\nprint stats[0]\nstatistics \u003d stats[1]\nprint statistics.keys()\nprint statistics[\u0027mean\u0027]\nprint\nprint statistics[\u0027stddev\u0027]",
      "dateUpdated": "Jul 18, 2016 5:29:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468127316043_-474861368",
      "id": "20160709-220836_1712272780",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "(\u0027total\u0027,)\n[\u0027count\u0027, \u0027max\u0027, \u0027mean\u0027, \u0027stddev\u0027, \u0027min\u0027]\n[[ 1.71071776  1.70419721  1.70721091 ...,  1.67181365  1.71173147\n   1.71695886]\n [ 1.75731502  1.76671777  1.79305201 ...,  1.71356708  1.76435064\n   1.77629037]\n [ 1.81650954  1.81797255  1.8401753  ...,  1.80185201  1.81978626\n   1.84110132]\n ..., \n [ 1.92764916  2.04530599  2.11015803 ...,  1.86887349  1.86151339\n   1.87664292]\n [ 1.80669124  1.89282266  1.97403288 ...,  2.24746037  2.23186773\n   2.15705626]\n [ 2.05630449  1.99153983  1.69522796 ...,  2.45432139  2.40891731\n   2.3436236 ]]\n\n[[4.062027432292203 3.938648041289098 3.8901799193382494 ...,\n  4.089620010823532 4.182758919650443 4.167168814948604]\n [4.1066652226926115 4.041364334463884 4.002423324450882 ...,\n  4.186038999934586 4.261223053275914 4.239783001193591]\n [4.156992138351043 4.146601277740004 4.198737735360134 ...,\n  4.247588771657733 4.274054452468062 4.27237244582923]\n ..., \n [6.776581615440196 6.735096440841192 6.741324579010839 ...,\n  6.97926725949421 6.555820113085848 6.698693352899527]\n [6.7788964202693665 6.839049085961517 6.852077055994818 ...,\n  6.879709520451571 6.589390888155897 6.6296333045219855]\n [7.004927623054714 7.050576015371279 6.588195535502897 ...,\n  5.900585342604049 6.231391024560441 6.588975476968176]]\n"
      },
      "dateCreated": "Jul 9, 2016 10:08:36 PM",
      "dateStarted": "Jul 18, 2016 5:29:13 PM",
      "dateFinished": "Jul 18, 2016 5:29:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "write statistics to a netCDF file",
      "text": "%pyspark\noutFile \u003d \u0027/tmp/trmm_precip_stats_1998_2012.nc\u0027\ninputFile \u003d urlList[0]\nprint inputFile\ntmp \u003d writeStats(inputFile, variable, stats, outFile)",
      "dateUpdated": "Jul 18, 2016 5:29:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468127418181_-456054644",
      "id": "20160709-221018_1766819391",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 225, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 4, in \u003cmodule\u003e\n  File \"\u003cstring\u003e\", line 14, in writeStats\n  File \"netCDF4/_netCDF4.pyx\", line 1747, in netCDF4._netCDF4.Dataset.__init__ (netCDF4/_netCDF4.c:11574)\nRuntimeError: No such file or directory\n"
      },
      "dateCreated": "Jul 9, 2016 10:10:18 PM",
      "dateStarted": "Jul 18, 2016 5:29:37 PM",
      "dateFinished": "Jul 18, 2016 5:29:37 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Plot Routines",
      "text": "%pyspark\nplotStats(inputFile, stat, \u0027mean\u0027)",
      "dateUpdated": "Jul 18, 2016 8:51:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468886410197_685735544",
      "id": "20160718-170010_230299265",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 18, 2016 5:00:10 PM",
      "dateStarted": "Jul 18, 2016 5:29:37 PM",
      "dateFinished": "Jul 18, 2016 5:29:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Entire spark program as Four lines (split-map-reduce)",
      "text": "%pyspark\nurlsRDD \u003d sc.parallelize(urlsByKey, numSlices\u003d16)                                # partition months into tasks\nmerged \u003d urlsRDD.map(lambda urls: accumulate(urls, variable)).reduce(combine)    # map-reduce\nstats \u003d statsFromAccumulators(merged)                                            # compute total stats\nwriteStats(urlList[0], variable, stats, outFile)                                       # write to netCDF4 file",
      "dateUpdated": "Jul 18, 2016 5:29:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468126718213_459086200",
      "id": "20160709-215838_1750987633",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 225, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 4, in \u003cmodule\u003e\n  File \"\u003cstring\u003e\", line 14, in writeStats\n  File \"netCDF4/_netCDF4.pyx\", line 1747, in netCDF4._netCDF4.Dataset.__init__ (netCDF4/_netCDF4.c:11574)\nRuntimeError: No such file or directory\n"
      },
      "dateCreated": "Jul 9, 2016 9:58:38 PM",
      "dateStarted": "Jul 18, 2016 5:29:37 PM",
      "dateFinished": "Jul 18, 2016 5:30:00 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003ch3\u003eExercise #1:\nWrite out and plot the monthly statistics, in addition to the total statistics ",
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468127940831_233750872",
      "id": "20160709-221900_1466833611",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003ch3\u003eExercise #1:\n\u003cbr  /\u003eWrite out and plot the monthly statistics, in addition to the total statistics\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 9, 2016 10:19:00 PM",
      "dateStarted": "Jul 18, 2016 5:29:14 PM",
      "dateFinished": "Jul 18, 2016 5:29:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Prepare smaller problem -- just Four months of data",
      "text": "urlListFile2 \u003d \"/data/cluster-local/bdwilson/Climatology/clim/urls_trmm_daily_2012.txt\"\n#urlListFile2 \u003d \"/home/workshop/urls_trmm_daily_2012.txt\"\nwith open(urlListFile2, \u0027r\u0027) as f:\n    urlList2 \u003d f.readlines()\nurlsByKey2 \u003d splitByMonth(urlList, {\u0027get\u0027: (\u0027year\u0027, \u0027month\u0027), \u0027regex\u0027: re.compile(r\u0027\\/3B42_daily.(....).(..)\u0027)})\nprint len(urlsByKey2)\nprint urlsByKey2[3]",
      "dateUpdated": "Jul 18, 2016 7:46:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468197741890_1479991291",
      "id": "20160710-174221_122029533",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\u003cconsole\u003e:30: error: not found: value urlListFile2\nval $ires6 \u003d urlListFile2\n             ^\n\u003cconsole\u003e:27: error: not found: value urlListFile2\n         urlListFile2 \u003d \"/data/cluster-local/bdwilson/Climatology/clim/urls_trmm_daily_2012.txt\"\n         ^\n"
      },
      "dateCreated": "Jul 10, 2016 5:42:21 PM",
      "dateStarted": "Jul 18, 2016 5:29:37 PM",
      "dateFinished": "Jul 18, 2016 5:29:38 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Exercise #1 Template:  Fill in the Blank Lines",
      "text": "%pyspark\nurlsRDD2  \u003d sc.parallelize(urlsByKey2, numSlices\u003d4)                  # partition months into tasks\naccumRDD2 \u003d urlsRDD2.map(lambda urls: accumulate(urls, variable))     # map accumulate\n                                                                     # three blank lines (it\u0027s easy!)\n\n\nmerged2 \u003d accumRDD2.reduce(combine)                                  # merge all months\nstats2 \u003d statsFromAccumulators(merged2)                              # compute total stats\nwriteStats(urls, variable, stats2, outFile)                          # write to total netCDF4 file",
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468195630584_-555933490",
      "id": "20160710-170710_1924555877",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 225, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 1, in \u003cmodule\u003e\nNameError: name \u0027urlsByKey2\u0027 is not defined\n"
      },
      "dateCreated": "Jul 10, 2016 5:07:10 PM",
      "dateStarted": "Jul 18, 2016 5:29:38 PM",
      "dateFinished": "Jul 18, 2016 5:30:00 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Exercise #1:  Hint",
      "text": "%pyspark\nurlsRDD2  \u003d sc.parallelize(urlsByKey, numSlices\u003d4)                   # partition months into tasks\naccumRDD2 \u003d urlsRDD2.map(lambda urls: accumulate(urls, variable))     # map accumulate\nmonthlyStatsRDD2 \u003d accumRDD.map(statsFromAccumulators)               # compute monthly stats in parallel\n                                                                     # write out monthly stats\n                                                                     # plot monthly stats\nmerged2 \u003d accumRDD2.reduce(combine)                                  # merge all months\nstats2 \u003d statsFromAccumulators(merged2)                              # compute total stats\nwriteStats(urls, variable, stats2, outFile)                          # write to total netCDF4 file",
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468196717575_-1292416217",
      "id": "20160710-172517_1239190597",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 76 cancelled part of cancelled job group zeppelin-20160710-172517_1239190597\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:405)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\n\n(\u003cclass \u0027py4j.protocol.Py4JJavaError\u0027\u003e, Py4JJavaError(u\u0027An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\\n\u0027, JavaObject id\u003do164), \u003ctraceback object at 0x7f5cf4a76dd0\u003e)"
      },
      "dateCreated": "Jul 10, 2016 5:25:17 PM",
      "dateStarted": "Jul 18, 2016 5:30:00 PM",
      "dateFinished": "Jul 18, 2016 5:30:35 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Exercise #1:  Bad Solution (use collect)",
      "text": "%pyspark\nurlsRDD2  \u003d sc.parallelize(urlsByKey2, numSlices\u003d4)                  # partition months into tasks\naccumRDD2 \u003d urlsRDD2.map(lambda urls: accumulate(urls, variable))    # map accumulate\nmonthlyStatsRDD2 \u003d accumRDD2.map(statsFromAccumulators)              # compute monthly stats in parallel\nmonthlyStatsList \u003d monthlyStatsRDD2.collect()                        # collect all monthly stats onto head node\nfor s in monthlyStatsList:\n   writeStats(urls, variable, stats, outFile)\n   plotStats(urls, variable)\n\nmerged2 \u003d accumRDD2.reduce(combine)                                  # merge all months\nstats2 \u003d statsFromAccumulators(merged2)                              # compute total stats\nwriteStats(urls, variable, stats2, outFile)                          # write to total netCDF4 file",
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468195637987_682973292",
      "id": "20160710-170717_1242358221",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 225, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 1, in \u003cmodule\u003e\nNameError: name \u0027urlsByKey2\u0027 is not defined\n"
      },
      "dateCreated": "Jul 10, 2016 5:07:17 PM",
      "dateStarted": "Jul 18, 2016 5:30:00 PM",
      "dateFinished": "Jul 18, 2016 5:30:35 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Exercise #1:  Better Solution (do in Parallel)",
      "text": "%pyspark\nurlsRDD2  \u003d sc.parallelize(urlsByKey2, numSlices\u003d4)                  # partition months into tasks\naccumRDD2 \u003d urlsRDD2.map(lambda urls: accumulate(urls, variable))    # map accumulate\nmonthlyStatsRDD2  \u003d accumRDD2.map(statsFromAccumulators)             # compute monthly stats in parallel\nmonthlyOutputRDD2 \u003d monthlyStatsRDD2.map(lambda stats: writeStats(urls, variable, stats, outFile, copyToHdfsPath\u003d\u0027/data\u0027))\nmonthlyPlotRDD2   \u003d  monthlyStatsRDD2.map(lambda stats: plotStats(urls, variable, stats, outFile, copyToHdfsPath\u003d\u0027/data\u0027))\n\nmerged2 \u003d accumRDD2.reduce(combine)                                  # merge all months\nstats2 \u003d statsFromAccumulators(merged2)                              # compute total stats\nwriteStats(urls, variable, stats2, outFile)                          # write to total netCDF4 file",
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorHide": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468195642824_-2125709003",
      "id": "20160710-170722_1089889959",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 225, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 1, in \u003cmodule\u003e\nNameError: name \u0027urlsByKey2\u0027 is not defined\n"
      },
      "dateCreated": "Jul 10, 2016 5:07:22 PM",
      "dateStarted": "Jul 13, 2016 7:04:43 PM",
      "dateFinished": "Jul 13, 2016 7:04:43 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003ch3\u003eExercise #2:  For the Reader\u003c/h3\u003e\nWrite out and plot the monthly and yearly statistics, in addition to the total statistics ",
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468198576138_887052197",
      "id": "20160710-175616_96294214",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eExercise #2:  For the Reader\u003c/h3\u003e\n\u003cp\u003eWrite out and plot the monthly and yearly statistics, in addition to the total statistics\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 10, 2016 5:56:16 PM",
      "dateStarted": "Jul 18, 2016 5:29:14 PM",
      "dateFinished": "Jul 18, 2016 5:29:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003ch3\u003eExercise #3:  Copy the Notebook and try it on your own Dataset\u003c/h3\u003e\nAgain, write out and plot the monthly and yearly statistics, in addition to the total statistics ",
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468198661739_-1516935551",
      "id": "20160710-175741_1673966523",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eExercise #3:  Copy the Notebook and try it on your own Dataset\u003c/h3\u003e\n\u003cp\u003eAgain, write out and plot the monthly and yearly statistics, in addition to the total statistics\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 10, 2016 5:57:41 PM",
      "dateStarted": "Jul 18, 2016 5:29:14 PM",
      "dateFinished": "Jul 18, 2016 5:29:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003ch3\u003eLessons Learned:\u003c/h3\u003e\n - Partition data as needed to main \"data locality\" for your algorithm (e.g. here monthly rather than random)\n - Write simple functions that you can pass to Map.\n - Try to use the longest pipeline of Map and Filter functions.\n - Only Reduce or ReduceByKey at the end of your pipeline.\n - Almost never use Collect (for truly BIG DATA, the result will be too big to collect).\n - Can use Collect to look at intermediate results to debug (by running on small subset of the data).\n - However, often better to write a sequential version of your code FIRST to debug.\n - HDFS is your friend.\n",
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468198757252_1182632942",
      "id": "20160710-175917_97440238",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eLessons Learned:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePartition data as needed to main \u0026ldquo;data locality\u0026rdquo; for your algorithm (e.g. here monthly rather than random)\u003c/li\u003e\n\u003cli\u003eWrite simple functions that you can pass to Map.\u003c/li\u003e\n\u003cli\u003eTry to use the longest pipeline of Map and Filter functions.\u003c/li\u003e\n\u003cli\u003eOnly Reduce or ReduceByKey at the end of your pipeline.\u003c/li\u003e\n\u003cli\u003eAlmost never use Collect (for truly BIG DATA, the result will be too big to collect).\u003c/li\u003e\n\u003cli\u003eCan use Collect to look at intermediate results to debug (by running on small subset of the data).\u003c/li\u003e\n\u003cli\u003eHowever, often better to write a sequential version of your code FIRST to debug.\u003c/li\u003e\n\u003cli\u003eHDFS is your friend.\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Jul 10, 2016 5:59:17 PM",
      "dateStarted": "Jul 18, 2016 5:29:14 PM",
      "dateFinished": "Jul 18, 2016 5:29:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003ch3\u003eFurther Generalizations:\u003c/h3\u003e\n - Add more statistics:  skewness, kurtosis\n - Adapt another algorithm for rollups:  linear detrending over time, area averaging with weighted interpolation, general climatologies, etc.\n - Adapt for algorithms that require the entire grid to be in memory\n - Try for 4-dimensional grids, (time, latitude, longitude, altitude)\n - Try for VERY HIGH resolution grids where Spark speedups will shine",
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468199482565_1638758482",
      "id": "20160710-181122_264262925",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eFurther Generalizations:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eAdd more statistics:  skewness, kurtosis\u003c/li\u003e\n\u003cli\u003eAdapt another algorithm for rollups:  linear detrending over time, area averaging with weighted interpolation, general climatologies, etc.\u003c/li\u003e\n\u003cli\u003eAdapt for algorithms that require the entire grid to be in memory\u003c/li\u003e\n\u003cli\u003eTry for 4-dimensional grids, (time, latitude, longitude, altitude)\u003c/li\u003e\n\u003cli\u003eTry for VERY HIGH resolution grids where Spark speedups will shine\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Jul 10, 2016 6:11:22 PM",
      "dateStarted": "Jul 18, 2016 5:29:14 PM",
      "dateFinished": "Jul 18, 2016 5:29:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\u003ch3\u003eAdvanced Exercise #3:  For the Reader\u003c/h3\u003e\nThink about how to parallelize not only over a long time-series, but over space and time simultaneously.\nSpatial tiles for very high-resolution grids anyone?",
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468199214061_-1232808618",
      "id": "20160710-180654_2052866075",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eAdvanced Exercise #3:  For the Reader\u003c/h3\u003e\n\u003cp\u003eThink about how to parallelize not only over a long time-series, but over space and time simultaneously.\n\u003cbr  /\u003eSpatial tiles for very high-resolution grids anyone?\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 10, 2016 6:06:54 PM",
      "dateStarted": "Jul 18, 2016 5:29:14 PM",
      "dateFinished": "Jul 18, 2016 5:29:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jul 18, 2016 5:29:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468459842210_-1677062300",
      "id": "20160713-183042_1299950450",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Jul 13, 2016 6:30:42 PM",
      "dateStarted": "Jul 13, 2016 7:04:43 PM",
      "dateFinished": "Jul 13, 2016 7:04:43 PM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "101-4 Parallel Statistical Rollups",
  "id": "2BS76DNCS",
  "angularObjects": {
    "2BATG925A": [],
    "2BCTKA5P2": [],
    "2B9VMB5BB": [],
    "2BAA8ZT1F": [],
    "2BCYFRWUC": [],
    "2BCC68R3T": [],
    "2BA8C2CJ4": [],
    "2B9AHSVAD": [],
    "2BCZV9QGQ": [],
    "2B9U51XQ6": [],
    "2B9VX5KPM": [],
    "2BAM6HXAB": [],
    "2B9AFX9BM": [],
    "2BS4GY9JZ": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}