{
  "paragraphs": [
    {
      "text": "%md\n## Please note this is part 2 of the End-to-End MCS example.\nIf you missed part 1, we suggest you start there!",
      "dateUpdated": "Jul 15, 2016 8:54:51 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468598007871_1146042679",
      "id": "20160715-085327_1473732656",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003ePlease note this is part 2 of the End-to-End MCS example.\u003c/h2\u003e\n\u003cp\u003eIf you missed part 1, we suggest you start there!\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 15, 2016 8:53:27 AM",
      "dateStarted": "Jul 15, 2016 8:54:43 AM",
      "dateFinished": "Jul 15, 2016 8:54:43 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\ncontinuing the algortihm... show image of where were are",
      "dateUpdated": "Jul 15, 2016 6:50:14 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468590559688_-1222629946",
      "id": "20160715-064919_1942085992",
      "dateCreated": "Jul 15, 2016 6:49:19 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n## Find the subgraphs\nThese subgraphs physically identify the cloudy areas that have evolve in time\nRight now we are reading the edgeList from a file which has to send to HDFS \nThere is a bit of cleaning of the file to be done before moving to HDFS. More specifically, remove \u0027List(\u0027 from the beginning of the file and last \u0027)\u0027 at the end of the file (these are from converting the List so as to print the outputs into the file).\nOnce this is done, the subgraphs part can be run. \n\nOur graph (the data object representing all the cloud elements and their connectivity through out the start and end times):\n* Sparse, disconnected and directed. \n* There are no cycles\n\nOur initial approach to determine all the connected subgraphs within this graph is:\n* Partition nodes such that possible connected nodes are together\n* On each partition: eliminate only subgraphs that are found in that partition \n* Merge consecutive partitions and repeat subgraph elimination\n* Continue until one partition remains OR all nodes are gone\n\nOur approach ensures a maximum number of iterations of log(paritions)/log(2) + 1\n\nOutstanding TODOs:\n* continued testing\n* allowing to read the edgelist directly from the RDD (ie. eliminate the text writing part)\n",
      "dateUpdated": "Jul 15, 2016 8:53:44 AM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468589108915_-1068766492",
      "id": "20160715-062508_163496936",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eFind the subgraphs\u003c/h2\u003e\n\u003cp\u003eThese subgraphs physically identify the cloudy areas that have evolve in time\n\u003cbr  /\u003eRight now we are reading the edgeList from a file which has to send to HDFS\n\u003cbr  /\u003eThere is a bit of cleaning of the file to be done before moving to HDFS. More specifically, remove \u0027List(\u0027 from the beginning of the file and last \u0027)\u0027 at the end of the file (these are from converting the List so as to print the outputs into the file).\n\u003cbr  /\u003eOnce this is done, the subgraphs part can be run.\u003c/p\u003e\n\u003cp\u003eOur graph (the data object representing all the cloud elements and their connectivity through out the start and end times):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSparse, disconnected and directed.\u003c/li\u003e\n\u003cli\u003eThere are no cycles\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOur initial approach to determine all the connected subgraphs within this graph is:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePartition nodes such that possible connected nodes are together\u003c/li\u003e\n\u003cli\u003eOn each partition: eliminate only subgraphs that are found in that partition\u003c/li\u003e\n\u003cli\u003eMerge consecutive partitions and repeat subgraph elimination\u003c/li\u003e\n\u003cli\u003eContinue until one partition remains OR all nodes are gone\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOur approach ensures a maximum number of iterations of log(paritions)/log(2) + 1\u003c/p\u003e\n\u003cp\u003eOutstanding TODOs:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003econtinued testing\u003c/li\u003e\n\u003cli\u003eallowing to read the edgelist directly from the RDD (ie. eliminate the text writing part)\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Jul 15, 2016 6:25:08 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "\nval edgesRDDn \u003d sc.parallelize(MCCEdgeList).map({ x \u003d\u003e s\"((${x.srcNode.frameNum},${x.srcNode.cloudElemNum}),(${x.destNode.frameNum},${x.destNode.cloudElemNum}))\"})\n\nval subGraphsCountn \u003dedgesRDDn.map(MainDistGraphMCC.mapFrames).groupByKey()\n// val subGraphsCountn \u003d edgesRDDn.map({x \u003d\u003e{\n//     val bins: Int \u003d 8\n//     val frames: Int \u003d 40\n//     val frameBucketSize: Float \u003d frames / bins\n//     val minAcceptableFeatureLength \u003d 3\n//     val maxIterations \u003d scala.math.ceil(scala.math.log(bins)/scala.math.log(2)) + 1\n    \n//     val nodes \u003d x.split(\"\\\\),\\\\(\")\n//     val source \u003d nodes(0).slice(2, nodes(0).length).split(\",\") // (FrameNum, CloudElemNum)\n//     val dest \u003d nodes(1).slice(0, nodes(1).length - 2).split(\",\") // (FrameNum, CloudElemNum)\n    // val nodeMap \u003d new mutable.HashMap[String, MCCNode]()\n    // val sourceFrameNum \u003d x.slice(2, x.indexOfSlice(\",\")).toInt\n    // val sourceKey \u003d source(0) + source(1)\n    // val destKey \u003d dest(0) + dest(1)\n    // if (!nodeMap.contains(sourceKey)) {\n    //   nodeMap.put(sourceKey, new MCCNode(source(0).toInt, source(1).toFloat))\n    // }\n    // if (!nodeMap.contains(destKey)) {\n    //   nodeMap.put(destKey, new MCCNode(dest(0).toInt, dest(1).toFloat))\n    // }\n    // val sourceNode: MCCNode \u003d nodeMap.get(sourceKey).get\n    // val destNode \u003d nodeMap.get(destKey).get\n\n    // for (i \u003c- 1 to bins) {\n    //   if (sourceFrameNum \u003c\u003d frameBucketSize * i) {\n    //     val bucket: Int \u003d i\n    //     (bucket,new MCCEdge(sourceNode, destNode))\n    //   }\n    // }\n    //  (bins, new MCCEdge(sourceNode, destNode))\n// }}).groupByKey()\n    MainDistGraphMCC.getSubgraphs(subGraphsCountn, 1)\n\n// val frameBucketSize \u003d 5\n\n// def mapEdgesToBuckets(edge: MCCEdge): (Integer, MCCEdge) \u003d {\n//     val sourceFrameNum \u003d edge.srcNode.frameNum\n//     val bucket \u003d scala.math.ceil(sourceFrameNum/frameBucketSize).toInt\n//     return (bucket, edge)\n// }\n//   println(MCCEdgeList.size)\n//   val frames \u003d new mutable.HashSet[Int]\n//   MCCEdgeList.foreach{edge \u003d\u003e {\n//     frames +\u003d (edge.srcNode.frameNum)\n//     frames +\u003d (edge.destNode.frameNum)\n//   }}\n//   println(frames.size)\n//   val edgesRDD \u003d sc.parallelize(MCCEdgeList).map(edge \u003d\u003e {\n//     val sourceFrameNum \u003d edge.srcNode.frameNum\n//     val bucket \u003d java.lang.Integer.valueOf(scala.math.ceil(sourceFrameNum/6).toInt)\n//     (bucket, edge)\n//   }).groupByKey()\n//   println(edgesRDD.collect().size)\n//   val bins: Int \u003d 8\n//     val frames: Int \u003d 40\n//     val frameBucketSize: Float \u003d frames / bins\n//     val minAcceptableFeatureLength \u003d 3\n//     val maxIterations \u003d scala.math.ceil(scala.math.log(bins)/scala.math.log(2)) + 1\n//   MainDistGraphMCC.getSubgraphs(edgesRDD,1)\n",
      "dateUpdated": "Jul 15, 2016 3:09:48 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468589108916_-1070690236",
      "id": "20160715-062508_8978043",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "546\nedgesRDD: org.apache.spark.rdd.RDD[(Integer, Iterable[org.dia.algorithms.mcc.MCCEdge])] \u003d ShuffledRDD[47] at groupByKey at \u003cconsole\u003e:94\n5\n"
      },
      "dateCreated": "Jul 15, 2016 6:25:08 AM",
      "dateStarted": "Jul 15, 2016 2:05:43 PM",
      "dateFinished": "Jul 15, 2016 2:15:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n## Step 10: Visualize the subgraphs\nFirst we need to place this data in a format that is useful for the visualization, then write it to a file. ",
      "dateUpdated": "Jul 15, 2016 6:25:08 AM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorHide": true,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468589108916_-1070690236",
      "id": "20160715-062508_570113411",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eVisualize the subgraphs\u003c/h2\u003e\n\u003cp\u003eFirst we need to place this data in a format that is useful for the visualization, then write it to a file.\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 15, 2016 6:25:08 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val sg \u003d subGraphsCountn.flatMap(t\u003d\u003e t._2)\nval sg1 \u003d sg.map({ t \u003d\u003e (\"\u0027F\"+t.toString.split(\",\")(0).split(\",\")(0).drop(2)+\"CE\"+t.toString.split(\",\")(1).split(\",\")(0).dropRight(4)+\"\u0027\", \"\u0027F\"+t.toString.split(\",\")(2).split(\",\")(0).drop(2)+\"CE\"+t.toString.split(\",\")(3).dropRight(4)+\"\u0027\")})\n\nz.put(\"graphEdgesVis\", sg1.collect().toList)\n// val d3Vis \u003d new PrintWriter(new File(\"graphEdgesVis.txt\"))\n// d3Vis.write(sg1.collect().toList + \"\\n\")\n// d3Vis.close()\n",
      "dateUpdated": "Jul 15, 2016 9:53:50 AM",
      "config": {
        "enabled": true,
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468589108916_-1070690236",
      "id": "20160715-062508_1971549410",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "sg: org.apache.spark.rdd.RDD[org.dia.algorithms.mcc.MCCEdge] \u003d MapPartitionsRDD[139] at flatMap at \u003cconsole\u003e:60\nsg1: org.apache.spark.rdd.RDD[(String, String)] \u003d MapPartitionsRDD[140] at map at \u003cconsole\u003e:62\nres449: Object \u003d Array((\u0027F2006080110CE89280\u0027,\u0027F2006080111CE320\u0027), (\u0027F2006080110CE112960\u0027,\u0027F2006080111CE320\u0027), (\u0027F2006080110CE843350\u0027,\u0027F2006080111CE1010\u0027), (\u0027F2006080110CE113280\u0027,\u0027F2006080111CE320\u0027), (\u0027F2006080110CE211696\u0027,\u0027F2006080111CE524\u0027), (\u0027F2006080110CE289050\u0027,\u0027F2006080111CE615\u0027), (\u0027F2006080110CE1706100\u0027,\u0027F2006080111CE1410\u0027), (\u0027F2006080110CE87680\u0027,\u0027F2006080111CE320\u0027), (\u0027F2006080110CE4165\u0027,\u0027F2006080111CE85\u0027), (\u0027F2006080110CE106896\u0027,\u0027F2006080111CE408\u0027), (\u0027F2006080110CE328548\u0027,\u0027F2006080111CE524\u0027), (\u0027F2006080110CE98560\u0027,\u0027F2006080111CE320\u0027), (\u0027F2006080110CE306540\u0027,\u0027F2006080111CE524\u0027), (\u0027F2006080110CE1112010\u0027,\u0027F2006080111CE1010\u0027), (\u0027F2006080110CE2606040\u0027,\u0027F2006080111CE1710\u0027), (\u0027F2006080110CE49036\u0027,\u0027F2006080111CE299\u0027), (\u0027F2006080110CE93170\u0027,\u0027F2006080111CE385\u0027), (\u0027F2006080110CE1680563\u0027,\u0027F2..."
      },
      "dateCreated": "Jul 15, 2016 6:25:08 AM",
      "dateStarted": "Jul 15, 2016 9:53:50 AM",
      "dateFinished": "Jul 15, 2016 9:53:51 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n# edit the file by removing the \u0027List(\u0027 from the beginning  and last \u0027)\u0027, then add \u0027[\u0027 at beginning and \u0027]\u0027 at the end for the Python script\nsed -i \u0027s/^.\\{,5\\}//; s/.$//\u0027 graphEdgesVis.txt\nsed -i \u0027s/^/[/; s/$/]/\u0027 graphEdgesVis.txt\n\n",
      "dateUpdated": "Jul 15, 2016 9:49:26 AM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/sh",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468589108917_-1071074985",
      "id": "20160715-062508_158260660",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 15, 2016 6:25:08 AM",
      "dateStarted": "Jul 15, 2016 9:49:26 AM",
      "dateFinished": "Jul 15, 2016 9:49:26 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\ngraphVizPy \u003d z.get(\"graphEdgesVis\")\nprint graphVizPy",
      "dateUpdated": "Jul 15, 2016 11:22:50 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468601545350_1762446522",
      "id": "20160715-095225_1384252929",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "None\n"
      },
      "dateCreated": "Jul 15, 2016 9:52:25 AM",
      "dateStarted": "Jul 15, 2016 11:22:50 AM",
      "dateFinished": "Jul 15, 2016 11:22:50 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nimport xmltodict, json, functools, sys\nimport networkx as nx\nfrom pprint import pprint\nimport graphviz as gv\n\ndef add_nodes(graph, nodes):\n    for n in nodes:\n        if isinstance(n, tuple):\n            graph.node(n[0], **n[1])\n        else:\n            graph.node(n) #_attributes\u003d{\"style\":\"filled\", \"fillcolor\":\"white\", \"tooltip\": \"brightness field\"}\n    return graph\n\n\ndef add_edges(graph, edges):\n    for e in edges:\n        if isinstance(e[0], tuple):\n            graph.edge(*e[0], **e[1])\n        else:\n            graph.edge(*e)\n    return graph\n\n\ndef main(argFile, maxLimit, edgeList): #tuples):\n   # inF \u003d open(edgeList, \u0027r\u0027)\n    #tuples \u003d eval(inF.readline())\n    #inF.close()\n    tuples \u003d [ eval(tup) for tup in edgeList ]\n\n    G \u003d nx.DiGraph(tuples)\n    weakConnComponents \u003d []\n    for w in nx.weakly_connected_component_subgraphs(G):\n        weakConnComponents.append(w.edges())\n\n    weakConnComponents.sort(key\u003dlambda x: len(x), reverse\u003dTrue)\n\n    #traverse weak component\n    \"\"\"\n    for w in nx.weakly_connected_component_subgraphs(G):\n    sorted(nx.weakly_connected_component_subgraphs(G), key\u003dlen, reverse \u003d True)]\n    \"\"\"\n\n    # iterate through all weakConnComponents here, lay them out one on top of another\n    yPos \u003d 0\n    for i in range(0, int(maxLimit)): #len(weakConnComponents)\n\n        digraph \u003d functools.partial(gv.Digraph, graph_attr\u003d{\"rankdir\": \"LR\", }, format\u003d\u0027svg\u0027) #\"ordering\":\"out\"\n        nodes \u003d set()\n        for edgeTup in weakConnComponents[i]:\n            nodes.add(edgeTup[0])\n            nodes.add(edgeTup[1])\n        nodes \u003d list(nodes)\n        edges \u003d weakConnComponents[i]\n\n\n        add_edges(add_nodes(digraph(), nodes), edges).render(\u0027/data/cluster-local/notebook_data/img/g\u0027 + str(i))\n\n\n        with open(\u0027/data/cluster-local/notebook_data/img/g{0}.svg\u0027.format(i)) as fd:\n            doc \u003d xmltodict.parse(fd.read())\n\n            nodeYLocation \u003d {}\n            for elem in doc[\"svg\"][\"g\"][\"g\"]:\n                if elem[\"@class\"] \u003d\u003d \"node\":\n                    #(Cx, Cy) x, y coordinates of center, origin top left corner\n                    #(Rx, Ry) radius of ellipse along x, y axes\n                    nodeYLocation[elem[\"title\"]] \u003d int(elem[\"ellipse\"][\"@cy\"])\n\n            #minNode \u003d max(nodeYLocation, key\u003dnodeYLocation.get)\n            #maxNode \u003d min(nodeYLocation, key\u003dnodeYLocation.get)\n            #diff \u003d nodeYLocation[minNode] - nodeYLocation[maxNode]\n            #print diff/9.0\n\n\n            #for every 24 change in y coordinate, place a node, in json\n            for key in nodeYLocation:\n                nodeYLocation[key] /\u003d -9\n                nodeYLocation[key] +\u003d yPos\n\n            yPos +\u003d nodeYLocation[max(nodeYLocation, key\u003dnodeYLocation.get)]\n\n            #print yPos\n\n            # assert len(nodeYLocation.keys()) len(nodes)\n\n            jsonData \u003d None\n            tempList \u003d []\n            with open(argFile) as jsonF:\n                jsonData \u003d eval(jsonF.read().split(\"girls \u003d \")[-1])\n\n                for lineDict in jsonData:\n                    for node in lineDict[\"values\"]:\n\n                        tempList.append(node[\"name\"])\n                        try:\n                            node[\"position\"] \u003d nodeYLocation[node[\"name\"]] + tempList.count(node[\"name\"]) * 0.0001\n                        except KeyError:\n                            continue\n\n\n\n            with open(argFile, \u0027w\u0027) as outF:\n                json.dump(jsonData, outF, indent\u003d4, sort_keys\u003dTrue, separators\u003d(\u0027,\u0027, \u0027: \u0027))\n\n            data \u003d None\n            with open(argFile, \"r\") as original:\n                data \u003d original.read().lstrip(\"[\")\n\n            with open(argFile, \"w\") as modified:\n                modified.write(\"var girls \u003d [\\n\" + data)",
      "dateUpdated": "Jul 15, 2016 11:22:11 AM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468589108917_-1071074985",
      "id": "20160715-062508_1302157634",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jul 15, 2016 6:25:08 AM",
      "dateStarted": "Jul 15, 2016 11:22:11 AM",
      "dateFinished": "Jul 15, 2016 11:22:11 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nnum_subgraphs \u003d 5\ngraphVizPy \u003d z.get(\"graphEdgesVis\")\n#main(\"/data/cluster-local/notebook_data/girl_name_us.js\", 5,\"/data/cluster-local/software/zeppelin-0.5.6-incubating-bin-all/bin/graphEdgesVis.txt\")\n\nmain(\"/data/cluster-local/notebook_data/girl_name_us.js\", 5, graphVizPy)\nfor i in range(num_subgraphs):\n    print \"\"\"%html \u003ch1\u003e Sub-Graph: \"\"\" + str(i) + \"\"\"\u003c/h1\u003e\"\"\"\n    with open(\"/data/cluster-local/notebook_data/img/g{0}.svg\".format(i),\"r\") as f:\n        svg_files_data \u003d f.read()\n        print \"\"\"%html \u003cspan id\u003d\"container\"\u003e\"\"\" + svg_files_data + \"\"\"\u003c/span\u003e\"\"\"",
      "dateUpdated": "Jul 15, 2016 11:22:19 AM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468589108917_-1071074985",
      "id": "20160715-062508_357508061",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark.py\", line 225, in \u003cmodule\u003e\n    eval(compiledCode)\n  File \"\u003cstring\u003e\", line 3, in \u003cmodule\u003e\n  File \"\u003cstring\u003e\", line 20, in main\nTypeError: \u0027NoneType\u0027 object is not iterable\n"
      },
      "dateCreated": "Jul 15, 2016 6:25:08 AM",
      "dateStarted": "Jul 15, 2016 11:22:19 AM",
      "dateFinished": "Jul 15, 2016 11:22:19 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\n",
      "dateUpdated": "Jul 15, 2016 6:25:08 AM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468589108940_-1092236175",
      "id": "20160715-062508_2126220568",
      "dateCreated": "Jul 15, 2016 6:25:08 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "201-4 End-to-end MCS part 2",
  "id": "2BQ7J1433",
  "angularObjects": {
    "2BATG925A": [],
    "2BCTKA5P2": [],
    "2B9VMB5BB": [],
    "2BAA8ZT1F": [],
    "2BCYFRWUC": [],
    "2BCC68R3T": [],
    "2BA8C2CJ4": [],
    "2B9AHSVAD": [],
    "2BCZV9QGQ": [],
    "2B9U51XQ6": [],
    "2B9VX5KPM": [],
    "2BAM6HXAB": [],
    "2B9AFX9BM": [],
    "2BQMW3S3P": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}