{
  "paragraphs": [
    {
      "text": "%md\n###Summary of some Lessons Learned and/or Spark Idioms\nWe go over simple examples of some Spark idioms you saw today:\n - Using Re-Keying and GroupByKey to Replicate Data -- Pairs of two time epochs\n - Use Accumulators instead of Multiple Reduces or Collects\n - In Spark shells or Notebooks, Global Varables won\u0027t be pulled into Lambda expressions, so use Literals\n - Must make an SQLContext to use Sparl SQL or DataFrames\n",
      "dateUpdated": "Jul 18, 2016 3:38:46 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468541581983_1093002041",
      "id": "20160714-171301_816673435",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eSummary of some Lessons Learned and/or Spark Idioms\u003c/h3\u003e\n\u003cp\u003eWe go over simple examples of some Spark idioms you saw today:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUsing Re-Keying and GroupByKey to Replicate Data \u0026ndash; Pairs of two time epochs\u003c/li\u003e\n\u003cli\u003eUse Accumulators instead of Multiple Reduces or Collects\u003c/li\u003e\n\u003cli\u003eIn Spark shells or Notebooks, Global Varables won\u0027t be pulled into Lambda expressions, so use Literals\u003c/li\u003e\n\u003cli\u003eMust make an SQLContext to use Sparl SQL or DataFrames\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Jul 14, 2016 5:13:01 PM",
      "dateStarted": "Jul 15, 2016 4:18:26 PM",
      "dateFinished": "Jul 15, 2016 4:18:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n###Pairing consecutive elements in a time series",
      "dateUpdated": "Jul 18, 2016 3:39:25 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468541920566_-1771547110",
      "id": "20160714-171840_833374459",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003ePairing consecutive elements in a time series\u003c/h3\u003e\n"
      },
      "dateCreated": "Jul 14, 2016 5:18:40 PM",
      "dateStarted": "Jul 14, 2016 5:24:10 PM",
      "dateFinished": "Jul 14, 2016 5:24:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "groupBy Approach",
      "text": "val consecutive \u003d sc.parallelize(0 to 10)\n                    .flatMap(p \u003d\u003e Array((p + 1, p), (p, p)))\n                    .groupBy(p \u003d\u003e p._1)\n                    .map(p \u003d\u003e p._2.map(e \u003d\u003e e._2).toList.sorted)\n                    .filter(p \u003d\u003e p.length !\u003d 1)\n                    .map(p \u003d\u003e (p(0), p(1)))\n                    .collect\n                    .sorted\n                    .toList",
      "dateUpdated": "Jul 17, 2016 10:21:09 AM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468541956046_-875420949",
      "id": "20160714-171916_1453448172",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "consecutive: List[(Int, Int)] \u003d List((0,1), (1,2), (2,3), (3,4), (4,5), (5,6), (6,7), (7,8), (8,9), (9,10))\n"
      },
      "dateCreated": "Jul 14, 2016 5:19:16 PM",
      "dateStarted": "Jul 16, 2016 11:00:31 PM",
      "dateFinished": "Jul 16, 2016 11:00:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Sliding function Approach",
      "text": "import org.apache.spark.mllib.rdd.RDDFunctions._\nval consecutive \u003d sc.parallelize(0 to 10)\n                    .sliding(2)\n                    .map(p \u003d\u003e (p(0),p(1)))\n                    .collect\n                    .toList\n                    ",
      "dateUpdated": "Jul 17, 2016 10:23:18 AM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468776082439_635398762",
      "id": "20160717-102122_1611170264",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.mllib.rdd.RDDFunctions._\nconsecutive: List[(Int, Int)] \u003d List((0,1), (1,2), (2,3), (3,4), (4,5), (5,6), (6,7), (7,8), (8,9), (9,10))\n"
      },
      "dateCreated": "Jul 17, 2016 10:21:22 AM",
      "dateStarted": "Jul 17, 2016 10:22:33 AM",
      "dateFinished": "Jul 17, 2016 10:22:35 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Use Accumulators to avoid multiple collects\nNormally to calculate the sume you can to a reduce to sum up the values and a reduce to count them up.\nTwo reduces are not necessary. This is the equivalent of running two for loops, when you only needed one.\nInstead use accumulators. \n\nIn the example we show that by mapping each value to a count number, we can perform an aggregate sum of both numbers in one step.\nWe can do the actual division operation once we have sum and count to get the mean.",
      "dateUpdated": "Jul 15, 2016 11:59:04 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468542243631_80657642",
      "id": "20160714-172403_1390277893",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eUse Accumulators to avoid multiple collects\u003c/h3\u003e\n\u003cp\u003eNormally to calculate the sume you can to a reduce to sum up the values and a reduce to count them up.\n\u003cbr  /\u003eTwo reduces are not necessary. This is the equivalent of running two for loops, when you only needed one.\n\u003cbr  /\u003eInstead use accumulators.\u003c/p\u003e\n\u003cp\u003eIn the example we show that by mapping each value to a count number, we can perform an aggregate sum of both numbers in one step.\n\u003cbr  /\u003eWe can do the actual division operation once we have sum and count to get the mean.\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 14, 2016 5:24:03 PM",
      "dateStarted": "Jul 15, 2016 11:59:04 AM",
      "dateFinished": "Jul 15, 2016 11:59:04 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val (sum, count) \u003d sc.parallelize(0 to 1000).map(p \u003d\u003e (p, 1)).reduce((A, B) \u003d\u003e (A._1 + B._1, A._2 + B._2))\nval mean \u003d sum / count",
      "dateUpdated": "Jul 14, 2016 6:06:10 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468544557365_1692607978",
      "id": "20160714-180237_1366668446",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "sum: Int \u003d 500500\ncount: Int \u003d 1001\nmean: Int \u003d 500\n"
      },
      "dateCreated": "Jul 14, 2016 6:02:37 PM",
      "dateStarted": "Jul 14, 2016 6:06:10 PM",
      "dateFinished": "Jul 14, 2016 6:07:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Function definitions in lambda expression\nFunctions can be passed into map task/ lambda expression. It is important to ensure that all dependencies (data and library) are accessible within the lambda expression. Failure usually results in **serializable errors**.\nFor example a class or object in scala that does not implement the serializable trait results in the error. ",
      "dateUpdated": "Jul 18, 2016 3:38:52 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468544632330_1741345206",
      "id": "20160714-180352_768545030",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eFunction definitions in lambda expression\u003c/h3\u003e\n\u003cp\u003eFunctions can be passed into map task/ lambda expression. It is important to ensure that all dependencies (data and library) are accessible within the lambda expression. Failure usually results in \u003cstrong\u003eserializable errors\u003c/strong\u003e.\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 14, 2016 6:03:52 PM",
      "dateStarted": "Jul 15, 2016 11:45:54 AM",
      "dateFinished": "Jul 15, 2016 11:45:54 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Does not Extend Serializable Trait",
      "text": "class NotSerializable(val k: Int){ override def toString() \u003d k.toString}\n\nsc.parallelize(1 to 10)\n.map(p \u003d\u003e new NotSerializable(p))\n.collect",
      "dateUpdated": "Jul 16, 2016 11:06:39 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468613800341_205712636",
      "id": "20160715-131640_970204898",
      "dateCreated": "Jul 15, 2016 1:16:40 PM",
      "dateStarted": "Jul 16, 2016 11:06:39 PM",
      "dateFinished": "Jul 16, 2016 11:06:40 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Extends Serializable trait",
      "text": "class IsSerializable(val k: Int) extends Serializable{ override def toString() \u003d k.toString}\n\nsc.parallelize(1 to 10)\n.map(p \u003d\u003e new IsSerializable(p))\n.collect",
      "dateUpdated": "Jul 16, 2016 11:06:43 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468735304462_-621937253",
      "id": "20160716-230144_1867251592",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class IsSerializable\nres115: Array[IsSerializable] \u003d Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
      },
      "dateCreated": "Jul 16, 2016 11:01:44 PM",
      "dateStarted": "Jul 16, 2016 11:06:43 PM",
      "dateFinished": "Jul 16, 2016 11:06:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Variables in Lambda Expressions in the Shell\nNormally in scala, the combination of assigning a sparkcontext to another variable and using  variables inside of a RDD map task works fine, as long as those objects are serializable.\nHowever in the shell (which is what zeppelin uses), does not allow this. It\u0027s a known issue in the spark-shell\nwhere using variables defined outside of a map task and used inside the lambda function throws a serializability error.\nFor this error to occur the spark context needs to have been assigned to a new variable.\nThe problem is that the shell tries to include the spark context as well, which is not serializable.",
      "dateUpdated": "Jul 18, 2016 3:38:59 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468613986585_854123447",
      "id": "20160715-131946_945262869",
      "dateCreated": "Jul 15, 2016 1:19:46 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Can\u0027t define Variables outside of map task in Shell",
      "text": "var k \u003d \"Hello\"\nval renamedSC \u003d sc\nval rdd \u003d renamedSC.parallelize(0 to 5)\n",
      "dateUpdated": "Jul 17, 2016 10:13:50 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": false,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468613986601_835655499",
      "id": "20160715-131946_1587552059",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "k: String \u003d Hello\nrenamedSC: org.apache.spark.SparkContext \u003d org.apache.spark.SparkContext@29984948\nrdd: org.apache.spark.rdd.RDD[Int] \u003d ParallelCollectionRDD[307] at parallelize at \u003cconsole\u003e:69\n"
      },
      "dateCreated": "Jul 15, 2016 1:19:46 PM",
      "dateStarted": "Jul 17, 2016 10:13:50 PM",
      "dateFinished": "Jul 17, 2016 10:13:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "rdd.map(p \u003d\u003e k + \" World\").collect// breaks\n\n\n\n\n\n\n\n",
      "dateUpdated": "Jul 17, 2016 10:14:02 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468614644766_904815591",
      "id": "20160715-133044_1019461344",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.SparkException: Task not serializable\n\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304)\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2055)\n\tat org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:323)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.map(RDD.scala:323)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:74)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:79)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:81)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:83)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:85)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:87)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:89)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:91)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:93)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:95)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:97)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:99)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:101)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:103)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:105)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:107)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:109)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:111)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:113)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:115)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:117)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:119)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:121)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:123)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:125)\n\tat \u003cinit\u003e(\u003cconsole\u003e:127)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:131)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:709)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:674)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:667)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:300)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:169)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:134)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.NotSerializableException: org.apache.spark.SparkContext\nSerialization stack:\n\t- object not serializable (class: org.apache.spark.SparkContext, value: org.apache.spark.SparkContext@29984948)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: renamedSC, type: class org.apache.spark.SparkContext)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@625f700b)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@4ae8394b)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@50a4c97d)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@5f16856f)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@63b9d4cf)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@757946d3)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@b8d9376)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@9ae6817)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@2e080573)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@34842bae)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@1f82e8d8)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@dc78328)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@54dd3d5a)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@2ee3d98e)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@6364cf99)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@1882dae6)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@132e1ca6)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@2c4afe61)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@791d1b97)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC@731c2765)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC@60c169ca)\n\t- field (class: $iwC$$iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC@42b88851)\n\t- field (class: $iwC$$iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC, $iwC$$iwC$$iwC@ea3c4f9)\n\t- field (class: $iwC$$iwC, name: $iw, type: class $iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC, $iwC$$iwC@6ec351b4)\n\t- field (class: $iwC, name: $iw, type: class $iwC$$iwC)\n\t- object (class $iwC, $iwC@227bd2d3)\n\t- field (class: $line339.$read, name: $iw, type: class $iwC)\n\t- object (class $line339.$read, $line339.$read@5a0caa04)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $VAL1167, type: class $line339.$read)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@39737996)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: $outer, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@358103c0)\n\t- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1, name: $outer, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\n\t- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1, \u003cfunction1\u003e)\n\tat org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)\n\tat org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301)\n\t... 64 more\n\n"
      },
      "dateCreated": "Jul 15, 2016 1:30:44 PM",
      "dateStarted": "Jul 17, 2016 10:13:52 PM",
      "dateFinished": "Jul 17, 2016 10:13:53 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Simplify the code to Not Use variables defined in the shell",
      "text": "renamedSC.parallelize(0 to 5).map(p \u003d\u003e \"Hello World\").collect",
      "dateUpdated": "Jul 17, 2016 10:13:07 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468614403184_-71417130",
      "id": "20160715-132643_2132061850",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res134: Array[String] \u003d Array(Hello World, Hello World, Hello World, Hello World, Hello World, Hello World)\n"
      },
      "dateCreated": "Jul 15, 2016 1:26:43 PM",
      "dateStarted": "Jul 17, 2016 10:13:07 PM",
      "dateFinished": "Jul 17, 2016 10:13:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Use the @Transient Tag",
      "text": "@transient val renamedSC \u003d sc\nval rdd \u003d renamedSC.parallelize(0 to 5)\nrdd.map(p \u003d\u003e k + \" World\").collect",
      "dateUpdated": "Jul 17, 2016 10:13:10 PM",
      "config": {
        "colWidth": 6.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468818635549_-615790597",
      "id": "20160717-221035_448397835",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "renamedSC: org.apache.spark.SparkContext \u003d org.apache.spark.SparkContext@29984948\nrdd: org.apache.spark.rdd.RDD[Int] \u003d ParallelCollectionRDD[304] at parallelize at \u003cconsole\u003e:69\nres136: Array[String] \u003d Array(Hello World, Hello World, Hello World, Hello World, Hello World, Hello World)\n"
      },
      "dateCreated": "Jul 17, 2016 10:10:35 PM",
      "dateStarted": "Jul 17, 2016 10:13:10 PM",
      "dateFinished": "Jul 17, 2016 10:13:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n### Using dataframes\nThe **hiveContext** is loaded by default in both Spark and Zeppelin. However, to use dataframes, the **sqlContext** is required. Be sure to check your configuration options:\nIn Zeppelin config\n```\nexport ZEPPELIN_SPARK_USEHIVECONTEXT  # Use HiveContext instead of SQLContext if set true. true by default.\n```\n\nIn Spark follow as below (Scala implementation shown):\n```\nval sc: SparkContext // An existing SparkContext.\nval sqlContext \u003d new org.apache.spark.sql.SQLContext(sc)\n```",
      "dateUpdated": "Jul 15, 2016 11:43:03 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468605465352_-1975396741",
      "id": "20160715-105745_507016284",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eUsing dataframes\u003c/h3\u003e\n\u003cp\u003eThe \u003cstrong\u003ehiveContext\u003c/strong\u003e is loaded by default in both Spark and Zeppelin. However, to use dataframes, the \u003cstrong\u003esqlContext\u003c/strong\u003e is required. Be sure to check your configuration options:\n\u003cbr  /\u003eIn Zeppelin config\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport ZEPPELIN_SPARK_USEHIVECONTEXT  # Use HiveContext instead of SQLContext if set true. true by default.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn Spark follow as below (Scala implementation shown):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eval sc: SparkContext // An existing SparkContext.\nval sqlContext \u003d new org.apache.spark.sql.SQLContext(sc)\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Jul 15, 2016 10:57:45 AM",
      "dateStarted": "Jul 15, 2016 11:42:56 AM",
      "dateFinished": "Jul 15, 2016 11:42:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Exercise 1 : Compute the sliding average with a window size of 3\nCompute the sliding average of a list of numbers from 0 to 10 with window size 3. \nYour solution should be the list with elements 1,2,3,4,5,6,7,8,9",
      "dateUpdated": "Jul 18, 2016 3:39:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468608176695_1517446243",
      "id": "20160715-114256_1571513693",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eExercise 1 : Compute the sliding average with a window size of 3\u003c/h3\u003e\n\u003cp\u003eCompute the sliding average of a list of numbers from 0 to 10 with window size 3.\n\u003cbr  /\u003eYour solution should be the original list.\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 15, 2016 11:42:56 AM",
      "dateStarted": "Jul 17, 2016 10:16:32 PM",
      "dateFinished": "Jul 17, 2016 10:16:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sc.parallelize(0 to 10)\n  .sliding(3)\n  .map(p \u003d\u003e p.sum/p.length)\n  .collect\n  .toList",
      "dateUpdated": "Jul 17, 2016 10:19:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468818989954_-2098474940",
      "id": "20160717-221629_1871980162",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res148: List[Int] \u003d List(1, 2, 3, 4, 5, 6, 7, 8, 9)\n"
      },
      "dateCreated": "Jul 17, 2016 10:16:29 PM",
      "dateStarted": "Jul 17, 2016 10:19:13 PM",
      "dateFinished": "Jul 17, 2016 10:19:14 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Jul 17, 2016 10:21:15 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468819053707_2130740809",
      "id": "20160717-221733_383332234",
      "dateCreated": "Jul 17, 2016 10:17:33 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "301-? Lessons Learned",
  "id": "2BRWB7B2D",
  "angularObjects": {
    "2BATG925A": [],
    "2BCTKA5P2": [],
    "2B9VMB5BB": [],
    "2BAA8ZT1F": [],
    "2BCYFRWUC": [],
    "2BR153A1B": [],
    "2BCC68R3T": [],
    "2BA8C2CJ4": [],
    "2B9AHSVAD": [],
    "2BCZV9QGQ": [],
    "2B9U51XQ6": [],
    "2B9VX5KPM": [],
    "2BAM6HXAB": [],
    "2B9AFX9BM": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}