{
  "paragraphs": [
    {
      "text": "%md\n#Using the SparkSQL package for discovery within the Storm Database  \n##Overview:\nSpark provides the SparkSQL Package for interaction with data leveraging SQL functionality. This notebook is to demonstrate using Apache Spark on meteorological related CSV dataset for quick analysis. Though this data is not voluminous per say, but it meets other properties of big data including variety and validity issues. Additionally, it is the type of data that scientists in the Earth Sciences, and social scientists will come across as data records before the Information Age which are necessary for climate analysis for e.g. [NOAA\u0027s Climate Data Online](https://www.ncdc.noaa.gov/cdo-web/datasets).\n\nIn this notebook we will explore:\n* reading data from a CSV file\n* cleaning the data\n* extract the columns of interests leveraging dataframes and RDDs\n* performing simple analysis\n* visualizng the results\n\n\nData is taken from: [National Centers for Environmental Information Storm Database]( https://www.ncdc.noaa.gov/stormevents/ftp.jsp) for the years 1990 -2006 inclusive.",
      "dateUpdated": "Jul 14, 2016 6:56:08 PM",
      "config": {
        "enabled": true,
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467954549001_-1950289473",
      "id": "20160707-220909_62957885",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eUsing the SparkSQL package for discovery within the Storm Database\u003c/h1\u003e\n\u003ch2\u003eOverview:\u003c/h2\u003e\n\u003cp\u003eSpark provides the SparkSQL Package for interaction with data leveraging SQL functionality. This notebook is to demonstrate using Apache Spark on meteorological related CSV dataset for quick analysis. Though this data is not voluminous per say, but it meets other properties of big data including variety and validity issues. Additionally, it is the type of data that scientists in the Earth Sciences, and social scientists will come across as data records before the Information Age which are necessary for climate analysis for e.g. \u003ca href\u003d\"https://www.ncdc.noaa.gov/cdo-web/datasets\"\u003eNOAA\u0027s Climate Data Online\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn this notebook we will explore:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ereading data from a CSV file\u003c/li\u003e\n\u003cli\u003ecleaning the data\u003c/li\u003e\n\u003cli\u003eextract the columns of interests leveraging dataframes and RDDs\u003c/li\u003e\n\u003cli\u003eperforming simple analysis\u003c/li\u003e\n\u003cli\u003evisualizng the results\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eData is taken from: \u003ca href\u003d\"https://www.ncdc.noaa.gov/stormevents/ftp.jsp\"\u003eNational Centers for Environmental Information Storm Database\u003c/a\u003e for the years 1990 -2006 inclusive.\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 7, 2016 10:09:09 PM",
      "dateStarted": "Jul 14, 2016 6:56:05 PM",
      "dateFinished": "Jul 14, 2016 6:56:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n##What is SparkSQL\n* The SparkSQL package brings the native support for SQL to Spark, streamling the process of querying data stored in RDDs and/or dataframes. This level of abstraction permits intermixing SQL commands to import data from various sources, runs SQL queires over imported and exisiting data, and writes the data in formats for various structured database formats.\n* The SparkSQL package is based on the **DataFrame** abstraction - an extension of the Apache Spark RDD abstraction. More specifically, the DataFrame contains an RDD of row objects that represent a record. Similar to a database, the schema of these records are known - hence the data is structured. DataFrames thus store data in a more efficient manner than native RDDs.\n* DataFrames provide operations not available to the native RDD including the ability to run SQL queries and operations (e.g. joins). \n",
      "dateUpdated": "Jul 12, 2016 6:47:51 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468338781983_-478703743",
      "id": "20160712-085301_2090994548",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eWhat is SparkSQL\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThe SparkSQL package brings the native support for SQL to Spark, streamling the process of querying data stored in RDDs and/or dataframes. This level of abstraction permits intermixing SQL commands to import data from various sources, runs SQL queires over imported and exisiting data, and writes the data in formats for various structured database formats.\u003c/li\u003e\n\u003cli\u003eThe SparkSQL package is based on the \u003cstrong\u003eDataFrame\u003c/strong\u003e abstraction - an extension of the Apache Spark RDD abstraction. More specifically, the DataFrame contains an RDD of row objects that represent a record. Similar to a database, the schema of these records are known - hence the data is structured. DataFrames thus store data in a more efficient manner than native RDDs.\u003c/li\u003e\n\u003cli\u003eDataFrames provide operations not available to the native RDD including the ability to run SQL queries and operations (e.g. joins).\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Jul 12, 2016 8:53:01 AM",
      "dateStarted": "Jul 12, 2016 9:14:59 AM",
      "dateFinished": "Jul 12, 2016 9:14:59 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Import libraries",
      "text": "import org.apache.spark.sql.functions._\nimport java.sql.Timestamp\nimport java.io.StringReader\nimport au.com.bytecode.opencsv.CSVReader\nimport java.sql.Timestamp\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport java.util.TimeZone\nimport org.joda.time._\nimport org.joda.time.format._\nimport org.joda.time.format.DateTimeFormat\nimport org.joda.time.DateTime\nimport scala.util.matching.Regex",
      "dateUpdated": "Jul 15, 2016 4:28:48 AM",
      "config": {
        "enabled": true,
        "tableHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467954549002_-1949135226",
      "id": "20160707-220909_2016641385",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.sql.functions._\nimport java.sql.Timestamp\nimport java.io.StringReader\nimport au.com.bytecode.opencsv.CSVReader\nimport java.sql.Timestamp\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport java.util.TimeZone\nimport org.joda.time._\nimport org.joda.time.format._\nimport org.joda.time.format.DateTimeFormat\nimport org.joda.time.DateTime\nimport scala.util.matching.Regex\n"
      },
      "dateCreated": "Jul 7, 2016 10:09:09 PM",
      "dateStarted": "Jul 15, 2016 4:28:48 AM",
      "dateFinished": "Jul 15, 2016 4:28:52 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Load the csv data",
      "text": "val inputds \u003d sc.textFile(\"hdfs://localhost:9000/workshop_s3/101/StormEvents_details1990-2006.csv\")\nval ds1 \u003d inputds.map{ line \u003d\u003e\n    val reader \u003d new CSVReader(new StringReader(line));\n    reader.readNext();\n}\nds1.first()\nval resultds1 \u003d ds1.mapPartitionsWithIndex{(idx, iter) \u003d\u003e if (idx \u003d\u003d 0) iter.drop(1) else iter}",
      "dateUpdated": "Jul 15, 2016 4:28:55 AM",
      "config": {
        "enabled": true,
        "tableHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "title": true,
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467954549002_-1949135226",
      "id": "20160707-220909_2104840741",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "inputds: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[92] at textFile at \u003cconsole\u003e:72\nds1: org.apache.spark.rdd.RDD[Array[String]] \u003d MapPartitionsRDD[93] at map at \u003cconsole\u003e:74\nres48: Array[String] \u003d Array(199007, 31, 2155, 199007, 31, 2155, \"\", 9984134, ARIZONA, 4, 1990, July, Thunderstorm Wind, C, 13, MARICOPA, PHX, 31-JUL-90 21:55:00, CST, 31-JUL-90 21:55:00, 0, 0, 0, 0, 0, 0, \"\", 60, \"\", \"\", \"\", \"\", 0, 0, \"\", \"\", \"\", \"\", 0, \"\", \"\", 0, \"\", \"\", 33.42, -111.72, \"\", \"\", \"\", \"\", PUB)\nresultds1: org.apache.spark.rdd.RDD[Array[String]] \u003d MapPartitionsRDD[94] at mapPartitionsWithIndex at \u003cconsole\u003e:76\n"
      },
      "dateCreated": "Jul 7, 2016 10:09:09 PM",
      "dateStarted": "Jul 15, 2016 4:28:55 AM",
      "dateFinished": "Jul 15, 2016 4:28:56 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Clean data: \n### Font style of text\n* To avoid case (upper of lower) formatting of text during analysis, it is worthwhile to convert most, if not all, the textual data to one format (upper or lower). The guidelines for doing this for this data are followed according to the associated README file. When no guidance is provided, lowercase is used. \n\n### Combining columns during the subset \n  * From looking at the data, we know that **BEGIN_DATE_TIME** and **END_DATE_TIME** should be combined with the timezone (**CZ_TIMEZONE**), so this will be completed.\n  * Let\u0027s say we don\u0027t care whether the injuries are direct or indirect (and similar for the deaths). Then, we can combine the columns **INJURIES_DIRECT** and **INJURIES_INDIRECT** to make one column called **injuries**, and likewise **DEATHS_DIRECT** and **DEATHS_INDIRECT** to make on column called **deaths**. \n\n### Date formatting \n* The date that determined from combining the columns above, should be represented as a datatime object and not as a string in the data frame. Furthermore, remove the timezones and convert the time to UTC.\n\n### Converting datatypes in columns\n  * For the columns **DAMAGE_PROPERTY** and **DAMAGE_CROPS**, the data is in the format number+character e.g. 30K or 3.0M (where K \u003d $1000.00 and M\u003d$1000000.00). We wish to make these columns numeric - floats.\n\n### Consolidating text some columns\n* From the investigation, it was noted that some sources were repetitive in terms of type e.g. \u0027Dept of highways\u0027 and department of highways\u0027. This data can be \u0027cleaned\u0027 by combining these likely term into one term e.g. \u0027dept of highways\u0027\n* The data can be further \u0027cleaned\u0027 by making categories.  For example:\n    * public for \u0027general public\u0027, \u0027public\u0027 ; \n    * media for \u0027newspaper\u0027, \u0027bradcast media\u0027;\n    * amateur for \u0027amateur radio\u0027, \u0027trained spotter\u0027, \u0027airplane pilot\u0027, \u0027utility company\u0027, \u0027COOP observer\u0027, \u0027storm chaser\u0027; \n    * amateur official for \u0027law enforcement\u0027, \u0027coast guard\u0027, \u0027park/forest service\u0027,\u0027emergency manager\u0027, \u0027fire dept/ rescue\u0027, \u0027other federal agency\u0027, \u0027govt official\u0027, \u0027dept of highways\u0027; .\n    * automated for \u0027AWSS\u0027, \u0027ASOS\u0027, \u0027SNOTEL\u0027, \u0027RAWS\u0027, \u0027AWOS\u0027, \u0027Mesonet\u0027, \u0027CoCoRaHS\u0027, \u0027bouy\u0027, \u0027c-man\u0027;\n    * official for \u0027official nws obs\u0027, \u0027mariner\u0027;\n    * other for \u0027insurance company;\n    * unknown for \u0027unknown\u0027;\nThis process of making categories can be subjective, and of course, some information will be lost. \nWe will make two frames here, one with categories and the other without for comparison later.  \n\n",
      "dateUpdated": "Jul 13, 2016 9:09:07 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467954549002_-1949135226",
      "id": "20160707-220909_560216956",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eClean data:\u003c/h2\u003e\n\u003ch3\u003eFont style of text\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eTo avoid case (upper of lower) formatting of text during analysis, it is worthwhile to convert most, if not all, the textual data to one format (upper or lower). The guidelines for doing this for this data are followed according to the associated README file. When no guidance is provided, lowercase is used.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eCombining columns during the subset\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFrom looking at the data, we know that \u003cstrong\u003eBEGIN_DATE_TIME\u003c/strong\u003e and \u003cstrong\u003eEND_DATE_TIME\u003c/strong\u003e should be combined with the timezone (\u003cstrong\u003eCZ_TIMEZONE\u003c/strong\u003e), so this will be completed.\u003c/li\u003e\n\u003cli\u003eLet\u0027s say we don\u0027t care whether the injuries are direct or indirect (and similar for the deaths). Then, we can combine the columns \u003cstrong\u003eINJURIES_DIRECT\u003c/strong\u003e and \u003cstrong\u003eINJURIES_INDIRECT\u003c/strong\u003e to make one column called \u003cstrong\u003einjuries\u003c/strong\u003e, and likewise \u003cstrong\u003eDEATHS_DIRECT\u003c/strong\u003e and \u003cstrong\u003eDEATHS_INDIRECT\u003c/strong\u003e to make on column called \u003cstrong\u003edeaths\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eDate formatting\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThe date that determined from combining the columns above, should be represented as a datatime object and not as a string in the data frame. Furthermore, remove the timezones and convert the time to UTC.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eConverting datatypes in columns\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFor the columns \u003cstrong\u003eDAMAGE_PROPERTY\u003c/strong\u003e and \u003cstrong\u003eDAMAGE_CROPS\u003c/strong\u003e, the data is in the format number+character e.g. 30K or 3.0M (where K \u003d $1000.00 and M\u003d$1000000.00). We wish to make these columns numeric - floats.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003eConsolidating text some columns\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFrom the investigation, it was noted that some sources were repetitive in terms of type e.g. \u0027Dept of highways\u0027 and department of highways\u0027. This data can be \u0027cleaned\u0027 by combining these likely term into one term e.g. \u0027dept of highways\u0027\u003c/li\u003e\n\u003cli\u003eThe data can be further \u0027cleaned\u0027 by making categories.  For example:\u003cul\u003e\n\u003cli\u003epublic for \u0027general public\u0027, \u0027public\u0027 ;\u003c/li\u003e\n\u003cli\u003emedia for \u0027newspaper\u0027, \u0027bradcast media\u0027;\u003c/li\u003e\n\u003cli\u003eamateur for \u0027amateur radio\u0027, \u0027trained spotter\u0027, \u0027airplane pilot\u0027, \u0027utility company\u0027, \u0027COOP observer\u0027, \u0027storm chaser\u0027;\u003c/li\u003e\n\u003cli\u003eamateur official for \u0027law enforcement\u0027, \u0027coast guard\u0027, \u0027park/forest service\u0027,\u0027emergency manager\u0027, \u0027fire dept/ rescue\u0027, \u0027other federal agency\u0027, \u0027govt official\u0027, \u0027dept of highways\u0027; .\u003c/li\u003e\n\u003cli\u003eautomated for \u0027AWSS\u0027, \u0027ASOS\u0027, \u0027SNOTEL\u0027, \u0027RAWS\u0027, \u0027AWOS\u0027, \u0027Mesonet\u0027, \u0027CoCoRaHS\u0027, \u0027bouy\u0027, \u0027c-man\u0027;\u003c/li\u003e\n\u003cli\u003eofficial for \u0027official nws obs\u0027, \u0027mariner\u0027;\u003c/li\u003e\n\u003cli\u003eother for \u0027insurance company;\u003c/li\u003e\n\u003cli\u003eunknown for \u0027unknown\u0027;\n\u003cbr  /\u003eThis process of making categories can be subjective, and of course, some information will be lost.\n\u003cbr  /\u003eWe will make two frames here, one with categories and the other without for comparison later.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Jul 7, 2016 10:09:09 PM",
      "dateStarted": "Jul 13, 2016 4:49:13 PM",
      "dateFinished": "Jul 13, 2016 4:49:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Functions for cleaning the data",
      "text": "def dollarsConversion (y:String): Double \u003d {\n    var dollars:Double \u003d 0.0\n    try{\n        if (!y.isEmpty \u0026\u0026 (y.trim.takeRight(1) \u003d\u003d \"K\" || y.trim.takeRight(1) \u003d\u003d \"k\")){\n            dollars \u003d y.trim.dropRight(1).toDouble*1000.0\n        }else if (!y.isEmpty \u0026\u0026 (y.trim.takeRight(1) \u003d\u003d \"M\" || y.trim.takeRight(1) \u003d\u003d \"m\")){\n            dollars \u003d y.trim.dropRight(1).toDouble*1000000.0\n        }\n    } catch {\n        case _: Throwable \u003d\u003e None\n    }\n    dollars\n}\n\ndef cleanSource (s:String): String \u003d {\n    var cleaned \u003d \"\"\n    if (s.toLowerCase() contains \"high\") {\n        cleaned \u003d \"dept of highway\"\n    }else if ( s.toLowerCase() contains \"nws obs\"){\n        cleaned \u003d \"official nws obs\"\n    }else if (s.toLowerCase() contains \"fire\"){\n        cleaned \u003d \"fire/rescue\"\n    }else{\n        cleaned \u003d s\n    }\n    cleaned\n}\n\ndef convertTime (t1:String, tz1:String): String \u003d {\n    var tzone \u003d tz1 \n    val t1format \u003d new SimpleDateFormat(\"dd-MMM-yy HH:mm:ss zzzzzz\")\n    // val tformat \u003d new SimpleDateFormat(\"dd-MMM-yyyy HH:mm:ss\")\n    val tformat \u003d new SimpleDateFormat(\"YYYY-MM-dd HH:mm:ss\")\n    var t \u003d \"\"\n    val tzoneOpts \u003d List(\"AST\", \"EDT\", \"EST\", \"CST\", \"MST\", \"PST\", \"AKST\", \"HST\") \n    tzoneOpts.find(x \u003d\u003e x.take(1) \u003d\u003d tzone.take(1)) match{\n        case Some(sa: String) \u003d\u003e {\n            if (sa.take(1) \u003d\u003d \"E\"){\n                if (sa.take(2) \u003d\u003d \"ES\"){\n                    tzone \u003d \"EST\"\n                }else if (sa.take(2) \u003d\u003d \"ED\"){\n                    tzone \u003d \"EDT\"\n                }else{\n                    tzone \u003d \"UTC\"\n                }\n            }else{\n                tzone \u003d sa\n            }\n           }\n           case None \u003d\u003e {\n               tzone \u003d \"UTC\"\n           }\n        }\n       if (t1 \u003d\u003d \"\"){\n           t \u003d null\n        } \n        else {\n            val t2 \u003d t1format.parse(t1 + \" \"+ tzone)\n            t \u003d tformat.format(t2)\n        } \n    t\n}\n\ndef categories(cat:String): String \u003d cat match{\n    case off if off.matches(\"[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*NWS[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\") \u003d\u003e \"OFFICIAL\"\n    case pub if pub.matches(\"^PUBL[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\") || pub.matches(\"^911[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\") \u003d\u003e \"PUBLIC\"\n    case med if med.matches(\"[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*MEDIA[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||med.matches(\"[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*NEWS[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\") \u003d\u003e \"MEDIA\"\n    case ama if ama.matches(\"^RAD[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||ama.matches(\"^TRA[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||ama.matches(\"[A-Z]*[\\\\s]*PILOT\")||\n      ama.matches(\"[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*UTIL[A-z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||ama.matches(\"[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*CHAS[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||ama.matches(\"^COOP[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\") \u003d\u003e \"AMATEUR\"\n    case auto if auto.matches(\"^AW\")||auto.matches(\"^ASO\")||auto.matches(\"^SNO\")||auto.matches(\"^RAW\")||\n      auto.matches(\"^MES[A-Z]*$T\")||auto.matches(\"^C[\\\\s]*[^0-9]*[\\\\s]*M[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||auto.matches(\"[A-Z]*[\\\\s]*[^0-9]*[\\\\s]BO[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||auto.matches(\"^COC[A-Z]*$T\")||\n      auto.matches(\"^DRO[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||auto.matches(\"^RIV[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\") \u003d\u003e \"AUTOMATED\"\n    case amo if amo.matches(\"[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*HIGH*\")||amo.matches(\"[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*FIRE\")||\n      amo.matches(\"^COAS*[\\\\s]*[^0-9]*[\\\\s]*G[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||amo.matches(\"PARK[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||amo.matches(\"FOREST[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||\n      amo.matches(\"^EME[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||amo.matches(\"[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*FEDER[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||amo.matches(\"^G[A-Z]*$T[\\\\s]*O[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||\n      amo.matches(\"^LAW*\")||amo.matches(\"[A-Z]*[\\\\s]*POST[\\\\s]*O[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\")||amo.matches(\"[A-Z]+[\\\\s]*[^0-9]*[\\\\s]*OFFI[A-Z]*[\\\\s]*[^0-9]*[\\\\s]*[A-Z]*\") \u003d\u003e \"AMATEUR OFFICIAL\"\n    case _ \u003d\u003e cat\n}\n\ndef cleanEventType (etype:String) : String \u003d etype match{\n    case a if a.matches(\"^AVAL[A-Z]+\") \u003d\u003e \"AVALANCHE\"\n    case b if b.matches(\"^BLIZ[A-Z]+\") \u003d\u003e \"BLIZZARD\"\n    case c if c.matches(\"^C([A-Z]+) F([A-Z]+)\") \u003d\u003e \"COASTAL FLOODING\"\n    case co if co.matches(\"^C([A-Z]+[^0-9]*[A-Z]+)\") \u0026\u0026 co.contains(\"CHIL\") \u003d\u003e \"COLD/WIND CHILL\"\n    case ce if ce.matches(\"^EXT[A-Z]+[\\\\s]*C([A-Z]+[^0-9]*[A-Z]+)\") \u0026\u0026 ce.contains(\"CHIL\") \u003d\u003e \"EXTREME COLD/WIND CHILL\"\n    case d if d.matches(\"^DUST[^0-9]*[\\\\s]*D[A-Z]+\") \u003d\u003e \"DUST DEVIL\"\n    case de if de.matches(\"^DUST[^0-9]*[\\\\s]*S[A-Z]+\") \u003d\u003e \"DUST STORM\"\n    case e if e.matches(\"^EXCE[A-Z]+[\\\\s]*H[A-Z]+\") \u003d\u003e \"EXCESSIVE HEAT\"\n    case f if f.matches(\"^LANDS*\") \u003d\u003e \"DEBRIS FLOW\"\n    case ff if ff.matches(\"^FLA*\") \u003d\u003e \"FLASHFLOOD\"\n    case fl if fl.matches(\"^FLO*\") \u003d\u003e \"FLOOD\"\n    case fr if fr.matches(\"^FRO*\") || fr.contains(\"FRO\") \u003d\u003e \"FROST\"\n    case fu if fu.matches(\"^FUN*\") \u0026\u0026 fu.contains(\"CLO\") \u003d\u003e \"FUNNEL CLOUD\"\n    case g if g.matches(\"^MAR[A-Z]+[\\\\s]*HAIL\") \u003d\u003e \"MARINE HAIL\"\n    case h if h.matches(\"^HAIL*\") \u003d\u003e \"HAIL\"\n    case he if he.matches(\"^HEAT\") \u003d\u003e \"HEAT\"\n    case hr if hr.matches(\"^H[A-Z]+Y[\\\\s]*RAIN*\") \u003d\u003e \"HEAVY RAIN\"\n    case hs if hs.matches(\"^H[A-Z]+Y[\\\\s]*SNOW\") \u003d\u003e \"HEAVY SNOW\"\n    case hi if hi.matches(\"^H[A-Z]+H[\\\\s]*W[A-Z]+\") \u003d\u003e \"HIGH WIND\"\n    case hu if hu.matches(\"^HUR[A-Z]+[\\\\s]*[^0-9]*[A-Z]*[^0-9]*\") || hu.contains(\"TYP\") \u003d\u003e \"HURRICANE\"\n    case i if i.matches(\"^ICE[\\\\s]*ST[A-Z]*\") \u003d\u003e \"ICE STORM\"\n    case j if j.matches(\"^LAKE[\\\\s]*[^0-9][\\\\s]*[A-Z]*[\\\\s]*[A-Z]*\") \u003d\u003e \"LAKE-EFFECT SNOW\"\n    case l if l.matches(\"^LIG[A-Z]+G$\") \u003d\u003e \"LIGHTNING\"\n    case r if r.matches(\"^RIP\") \u003d\u003e \"RIP CURRENT\"\n    case s if s.matches(\"^SLEET\") \u003d\u003e \"SLEET\"\n    case t if t.matches(\"^TOR\") \u003d\u003e \"TORNADO\"\n    case th if th.matches(\"^THU[A-Z]+\")  \u003d\u003e \"THUNDERSTORM\"\n    case ts if ts.matches(\"^TSTM\") || ts.matches(\"^TR[A-Z]+[\\\\s]*S[A-Z]+\")  \u003d\u003e \"TROPICAL STORM\"\n    case v if v.matches(\"^VOL\") \u003d\u003e \"VOLCANIC ASH\"\n    case w if w.matches(\"^WAT\") \u003d\u003e \"WATERSPOUT\"\n    case wi if wi.matches(\"^WIN\") \u003d\u003e \"WINTER STORM\"\n    case \"\" \u003d\u003e \"\"\n    case _ \u003d\u003e etype\n}\n\n// assuming string instead of date\ndef mthToInt (mthStr:String) : Int \u003d{\n    val mth \u003d mthStr.toUpperCase() match{\n        case \"JAN\" \u003d\u003e 1\n        case \"FEB\" \u003d\u003e 2\n        case \"MAR\" \u003d\u003e 3\n        case \"APR\" \u003d\u003e 4\n        case \"MAY\" \u003d\u003e 5\n        case \"JUN\" \u003d\u003e 6\n        case \"JUL\" \u003d\u003e 7\n        case \"AUG\" \u003d\u003e 8\n        case \"SEP\" \u003d\u003e 9\n        case \"OCT\" \u003d\u003e 10\n        case \"NOV\" \u003d\u003e 11\n        case \"DEC\" \u003d\u003e 12\n    } \n    mth\n}",
      "dateUpdated": "Jul 15, 2016 4:29:01 AM",
      "config": {
        "enabled": true,
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "colWidth": 12.0,
        "lineNumbers": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467954549003_-1949519975",
      "id": "20160707-220909_998071044",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "dollarsConversion: (y: String)Double\ncleanSource: (s: String)String\nconvertTime: (t1: String, tz1: String)String\ncategories: (cat: String)String\ncleanEventType: (etype: String)String\nmthToInt: (mthStr: String)Int\n"
      },
      "dateCreated": "Jul 7, 2016 10:09:09 PM",
      "dateStarted": "Jul 15, 2016 4:29:02 AM",
      "dateFinished": "Jul 15, 2016 4:29:04 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "create the DataFrame using toDF()",
      "text": "case class Events(beginDate:String, endDate:String, state:String, eventType:String, czType:String, injuries:Int, deaths:Int, damageProp:Double, damageCrops:Double, source:String, category:String )\n\nval subsetDFds \u003d resultds1.map(c \u003d\u003e Events(convertTime(c(17).trim.toString, c(18).trim.toString.toUpperCase()),\n                                        convertTime(c(19).trim.toString, c(18).trim.toString.toUpperCase()),\n                                        c(8).trim.toString.toUpperCase(), \n                                        cleanEventType(c(12).trim.toString.toUpperCase()), \n                                        c(13).trim.toString.toUpperCase(), \n                                        c(20).toInt+ c(21).toInt,\n                                        c(22).toInt+c(23).toInt, \n                                        dollarsConversion(c(24).toString),     \n                                        dollarsConversion(c(25).toString),\n                                        cleanSource(c(26).trim.toString.toLowerCase()),\n                                        categories(c(26).trim.toString.toUpperCase()))).toDF()",
      "dateUpdated": "Jul 15, 2016 4:29:05 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "tableHide": true,
        "lineNumbers": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468360348534_1834353044",
      "id": "20160712-145228_1281429590",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "defined class Events\nsubsetDFds: org.apache.spark.sql.DataFrame \u003d [beginDate: string, endDate: string, state: string, eventType: string, czType: string, injuries: int, deaths: int, damageProp: double, damageCrops: double, source: string, category: string]\n"
      },
      "dateCreated": "Jul 12, 2016 2:52:28 PM",
      "dateStarted": "Jul 15, 2016 4:29:05 AM",
      "dateFinished": "Jul 15, 2016 4:29:07 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "subsetDFds.show(20)",
      "dateUpdated": "Jul 15, 2016 4:29:09 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468422831335_463759443",
      "id": "20160713-081351_762259838",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+-------------------+-------------------+-------------+-----------------+------+--------+------+----------+-----------+------+--------+\n|          beginDate|            endDate|        state|        eventType|czType|injuries|deaths|damageProp|damageCrops|source|category|\n+-------------------+-------------------+-------------+-----------------+------+--------+------+----------+-----------+------+--------+\n|1990-05-16 12:20:00|1990-05-16 12:20:00|     MISSOURI|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-12-21 09:07:00|1990-12-21 09:07:00|    LOUISIANA|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-04-01 17:12:00|1990-04-01 17:12:00|      ALABAMA|THUNDERSTORM WIND|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-03-12 19:47:00|1990-03-12 19:47:00|       KANSAS|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-07-15 17:07:00|1990-07-15 17:07:00|       KANSAS|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-05-21 11:07:00|1990-05-21 11:07:00|  MISSISSIPPI|THUNDERSTORM WIND|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-12 19:55:00|1990-06-12 19:55:00|     NEBRASKA|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-12 20:10:00|1990-06-12 20:10:00|     NEBRASKA|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-08 15:02:00|1990-06-08 15:02:00|       KANSAS|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-08 15:17:00|1990-06-08 15:17:00|       KANSAS|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-02 10:50:00|1990-06-02 10:50:00|    MINNESOTA|          TORNADO|     C|       0|     0|   25000.0|        0.0|      |        |\n|1990-05-18 09:10:00|1990-05-18 09:10:00|MASSACHUSETTS|          TORNADO|     C|       0|     0|    2500.0|        0.0|      |        |\n|1990-02-09 21:30:00|1990-02-09 21:30:00|  MISSISSIPPI|THUNDERSTORM WIND|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-12 20:20:00|1990-06-12 20:20:00|     NEBRASKA|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-12 20:30:00|1990-06-12 20:30:00|     NEBRASKA|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-12 20:39:00|1990-06-12 20:39:00|     NEBRASKA|             HAIL|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-12 20:45:00|1990-06-12 20:45:00|     NEBRASKA|          TORNADO|     C|       0|     0|    2500.0|        0.0|      |        |\n|1990-06-12 20:45:00|1990-06-12 20:45:00|     NEBRASKA|THUNDERSTORM WIND|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-12 21:15:00|1990-06-12 21:15:00|     NEBRASKA|THUNDERSTORM WIND|     C|       0|     0|       0.0|        0.0|      |        |\n|1990-06-12 21:20:00|1990-06-12 21:20:00|     NEBRASKA|          TORNADO|     C|       0|     0|   25000.0|        0.0|      |        |\n+-------------------+-------------------+-------------+-----------------+------+--------+------+----------+-----------+------+--------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Jul 13, 2016 8:13:51 AM",
      "dateStarted": "Jul 15, 2016 4:29:09 AM",
      "dateFinished": "Jul 15, 2016 4:29:11 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Register the DataFrame",
      "text": "val mthlyDFBeginds \u003d subsetDFds.withColumn(\"beginMth\", substring(subsetDFds(\"beginDate\"),4,3))\nval mthlyDFds \u003d mthlyDFBeginds.withColumn(\"endMth\", substring(mthlyDFBeginds(\"endDate\"),4,3))\n\nmthlyDFds.registerTempTable(\"stormData\")\nsqlc.udf.register(\"mthToInt\", mthToInt _)",
      "dateUpdated": "Jul 15, 2016 4:29:15 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468436518668_1549580114",
      "id": "20160713-120158_1632850607",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "mthlyDFBeginds: org.apache.spark.sql.DataFrame \u003d [beginDate: string, endDate: string, state: string, eventType: string, czType: string, injuries: int, deaths: int, damageProp: double, damageCrops: double, source: string, category: string, beginMth: string]\nmthlyDFds: org.apache.spark.sql.DataFrame \u003d [beginDate: string, endDate: string, state: string, eventType: string, czType: string, injuries: int, deaths: int, damageProp: double, damageCrops: double, source: string, category: string, beginMth: string, endMth: string]\nres63: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,IntegerType,List(StringType))\n"
      },
      "dateCreated": "Jul 13, 2016 12:01:58 PM",
      "dateStarted": "Jul 15, 2016 4:29:15 AM",
      "dateFinished": "Jul 15, 2016 4:29:17 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Simple querying of the data frame\nConsider the categories and the number of reports in each. Note that there are over 1million reports in this file.",
      "dateUpdated": "Jul 13, 2016 5:09:19 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467954549003_-1949519975",
      "id": "20160707-220909_1154274077",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eSimple querying of the data frame\u003c/h2\u003e\n\u003cp\u003eConsider the categories and the number of reports in each. Note that there are over 1million reports in this file.\u003c/p\u003e\n"
      },
      "dateCreated": "Jul 7, 2016 10:09:09 PM",
      "dateStarted": "Jul 13, 2016 5:09:15 PM",
      "dateFinished": "Jul 13, 2016 5:09:16 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Querying the dataFrame with \u0027typical\u0027 rDD operations",
      "text": "val numOffds \u003d subsetDFds.filter(subsetDFds(\"category\") \u003d\u003d\u003d \"OFFICIAL\").count\nval numAutods \u003d subsetDFds.filter(subsetDFds(\"category\") \u003d\u003d\u003d \"AUTOMATED\").count\nval numAmateurds \u003d subsetDFds.filter(subsetDFds(\"category\") \u003d\u003d\u003d \"AMATEUR\").count",
      "dateUpdated": "Jul 15, 2016 4:29:20 AM",
      "config": {
        "enabled": true,
        "tableHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1467954549004_-1951443720",
      "id": "20160707-220909_1664628788",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "numOffds: Long \u003d 87096\nnumAutods: Long \u003d 19563\nnumAmateurds: Long \u003d 240551\n"
      },
      "dateCreated": "Jul 7, 2016 10:09:09 PM",
      "dateStarted": "Jul 15, 2016 4:29:20 AM",
      "dateFinished": "Jul 15, 2016 4:31:47 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Querying the DataFrame leveraging SQL",
      "text": "val totReportds \u003d sqlContext.sql(\"SELECT COUNT(*) FROM stormData\").collect()\nval numAmOffds \u003d sqlContext.sql(\"SELECT COUNT(*) FROM stormData WHERE category \u003d \u0027AMATEUR OFFICIAL\u0027\").collect() \nval numPubds \u003d sqlContext.sql(\"SELECT COUNT(*) FROM stormData WHERE category \u003d \u0027PUBLIC\u0027\").collect() \nval numMedds \u003d sqlContext.sql(\"SELECT COUNT(*) FROM stormData WHERE category \u003d \u0027MEDIA\u0027\").collect() ",
      "dateUpdated": "Jul 15, 2016 4:32:56 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "tableHide": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468431722630_-46226910",
      "id": "20160713-104202_1160633844",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "totReportds: Array[org.apache.spark.sql.Row] \u003d Array([1211069])\nnumAmOffds: Array[org.apache.spark.sql.Row] \u003d Array([154206])\nnumPubds: Array[org.apache.spark.sql.Row] \u003d Array([69651])\nnumMedds: Array[org.apache.spark.sql.Row] \u003d Array([76410])\n"
      },
      "dateCreated": "Jul 13, 2016 10:42:02 AM",
      "dateStarted": "Jul 15, 2016 4:32:56 AM",
      "dateFinished": "Jul 15, 2016 4:36:04 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "",
      "text": "\nprintln(\"\\nNUM of official reports is \"+ numOffds + \". This is \"+ \"%04.2f\".format((numAmOffds(0).getLong(0).toFloat/totReportds(0).getLong(0))*100.0) + \"% of the total reports.\")\n\nprintln(\"\\n\"+ (totReportds(0).getLong(0) - numOffds.toInt).toString +\" reports were made from sources other than the NWS.\")\n\n\nprintln(\"\\nNUM of official amateur reports is \"+ numAmOffds(0).getLong(0).toString )\nprintln(\"\\nNUM of automated reports is \"+ numAutods.toString )\nprintln(\"\\nNUM OF amateur reports is \"+ numAmateurds.toString)\nprintln(\"\\nNUM OF public reports is \"+ numPubds(0).getLong(0).toString)\nprintln(\"\\nNUM OF media reports is \"+ numMedds(0).getLong(0).toString)",
      "dateUpdated": "Jul 15, 2016 4:46:27 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": false,
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468442354715_1822281120",
      "id": "20160713-133914_1439499658",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nNUM of official reports is 87096. This is 12.73% of the total reports.\n\n1123973 reports were made from sources other than the NWS.\n\nNUM of official amateur reports is 154206\n\nNUM of automated reports is 19563\n\nNUM OF amateur reports is 240551\n\nNUM OF public reports is 69651\n\nNUM OF media reports is 76410\n"
      },
      "dateCreated": "Jul 13, 2016 1:39:14 PM",
      "dateStarted": "Jul 15, 2016 4:46:27 AM",
      "dateFinished": "Jul 15, 2016 4:46:28 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Question: What are the top events In this file",
      "text": "val oo \u003d sqlContext.sql(\"SELECT DISTINCT eventType, COUNT(eventType) AS eventCount FROM stormData GROUP BY eventType\").sort(desc(\"eventCount\")).toDF()\noo.registerTempTable(\"top20Events\")\n",
      "dateUpdated": "Jul 15, 2016 4:33:36 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468432398537_-1860581250",
      "id": "20160713-105318_138109500",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "oo: org.apache.spark.sql.DataFrame \u003d [eventType: string, eventCount: bigint]\n"
      },
      "dateCreated": "Jul 13, 2016 10:53:18 AM",
      "dateStarted": "Jul 14, 2016 6:29:57 PM",
      "dateFinished": "Jul 14, 2016 6:29:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "oo.show(20)",
      "dateUpdated": "Jul 14, 2016 1:51:06 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "tableHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468434216399_2104891034",
      "id": "20160713-112336_370745775",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+--------------------+----------+\n|           eventType|eventCount|\n+--------------------+----------+\n|   THUNDERSTORM WIND|    311557|\n|                HAIL|    283675|\n|         FLASH FLOOD|     69408|\n|        WINTER STORM|     63051|\n|           HIGH WIND|     56963|\n|          HEAVY SNOW|     54288|\n|             DROUGHT|     48714|\n|      WINTER WEATHER|     47376|\n|               FLOOD|     41897|\n|             TORNADO|     34288|\n|MARINE THUNDERSTO...|     19153|\n|          HEAVY RAIN|     18612|\n|                HEAT|     17105|\n|         STRONG WIND|     16845|\n|           LIGHTNING|     15167|\n|     COLD/WIND CHILL|     12718|\n|            BLIZZARD|     11545|\n|           DENSE FOG|     10639|\n|           ICE STORM|     10340|\n|EXTREME COLD/WIND...|      9706|\n+--------------------+----------+\nonly showing top 20 rows\n\n"
      },
      "dateCreated": "Jul 13, 2016 11:23:36 AM",
      "dateStarted": "Jul 14, 2016 1:51:06 PM",
      "dateFinished": "Jul 14, 2016 1:51:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Top 10 events ",
      "text": "%sql\nselect * from top20Events limit 10",
      "dateUpdated": "Jul 15, 2016 4:33:43 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "eventType",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "eventCount",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "eventType",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "eventCount",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/sql",
        "editorHide": true,
        "title": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468529629253_-1239397671",
      "id": "20160714-135349_1124547095",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "eventType\teventCount\nTHUNDERSTORM WIND\t311557\nHAIL\t283675\nFLASH FLOOD\t69408\nWINTER STORM\t63051\nHIGH WIND\t56963\nHEAVY SNOW\t54288\nDROUGHT\t48714\nWINTER WEATHER\t47376\nFLOOD\t41897\nTORNADO\t34288\n"
      },
      "dateCreated": "Jul 14, 2016 1:53:49 PM",
      "dateStarted": "Jul 14, 2016 6:54:03 PM",
      "dateFinished": "Jul 14, 2016 6:55:06 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Question: What Are The Top 10 deadliest Events",
      "text": "val oo1 \u003d sqlContext.sql(\"SELECT eventType, SUM(deaths) as deathCount FROM stormData GROUP BY eventType ORDER BY deathCount\").sort(desc(\"deathCount\")).toDF()\noo1.registerTempTable(\"10deadliest\")\n",
      "dateUpdated": "Jul 15, 2016 4:33:58 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "title": true,
        "editorHide": false,
        "tableHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468547823712_2119692339",
      "id": "20160714-185703_777143327",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "oo1: org.apache.spark.sql.DataFrame \u003d [eventType: string, deathCount: bigint]\n"
      },
      "dateCreated": "Jul 14, 2016 6:57:03 PM",
      "dateStarted": "Jul 14, 2016 7:34:23 PM",
      "dateFinished": "Jul 14, 2016 7:34:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sql\nselect * from 10deadliest limit 10",
      "dateUpdated": "Jul 14, 2016 7:37:58 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "pieChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "eventType",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "deathCount",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "eventType",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/sql",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468548526502_1558920556",
      "id": "20160714-190846_1218800765",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "eventType\tdeathCount\nHEAT\t2200\nTORNADO\t1956\nFLASH FLOOD\t1250\nLIGHTNING\t791\nRIP CURRENT\t746\nTHUNDERSTORM WIND\t722\nWINTER WEATHER\t674\nEXCESSIVE HEAT\t511\nFLOOD\t484\nWINTER STORM\t449\n"
      },
      "dateCreated": "Jul 14, 2016 7:08:46 PM",
      "dateStarted": "Jul 14, 2016 7:34:27 PM",
      "dateFinished": "Jul 14, 2016 7:36:26 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Deadliest in 2000",
      "text": "val deathsByYr \u003d sqlContext.sql(\"SELECT YEAR(beginDate) as beginYr, eventType, deaths FROM stormData WHERE YEAR(beginDate) \u003d2000\").toDF()\ndeathsByYr.registerTempTable(\"2000deadliest\")",
      "dateUpdated": "Jul 15, 2016 4:36:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": true,
        "title": true,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468456977855_-400617865",
      "id": "20160713-174257_1114720950",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "deathsByYr: org.apache.spark.sql.DataFrame \u003d [beginYr: int, eventType: string, deaths: int]\n"
      },
      "dateCreated": "Jul 13, 2016 5:42:57 PM",
      "dateStarted": "Jul 14, 2016 8:04:04 PM",
      "dateFinished": "Jul 14, 2016 8:04:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Exercises",
      "text": "%md\n1. Find the top 3 deadliest events by year. \n2. Inspect the cleaning functions, how can they be improved?",
      "dateUpdated": "Jul 18, 2016 3:19:32 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "title": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468551349444_-323008384",
      "id": "20160714-195549_1835744124",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003col\u003e\n\u003cli\u003eFind the top 3 deadliest events by year.\u003c/li\u003e\n\u003cli\u003eInspect the cleaning functions, how can they be improved?\u003c/li\u003e\n\u003c/ol\u003e\n"
      },
      "dateCreated": "Jul 14, 2016 7:55:49 PM",
      "dateStarted": "Jul 18, 2016 3:19:30 AM",
      "dateFinished": "Jul 18, 2016 3:19:30 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1468582546193_-1412508650",
      "id": "20160715-043546_25274824",
      "dateCreated": "Jul 15, 2016 4:35:46 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "101-2 SparkSQL and DataFrames",
  "id": "2BRUBFVEX",
  "angularObjects": {
    "2BATG925A": [],
    "2BCTKA5P2": [],
    "2B9VMB5BB": [],
    "2BAA8ZT1F": [],
    "2BCYFRWUC": [],
    "2BCC68R3T": [],
    "2BA8C2CJ4": [],
    "2B9AHSVAD": [],
    "2BCZV9QGQ": [],
    "2B9U51XQ6": [],
    "2B9VX5KPM": [],
    "2BAM6HXAB": [],
    "2B9AFX9BM": [],
    "2BQMW3S3P": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}