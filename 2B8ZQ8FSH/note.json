{
  "paragraphs": [
    {
      "text": "%md\n#MCS Algorithm to-date in SciSpark\nPurpose: This notebook will demonstrate using SciSpark to identify and track mesoscale convective systems (MCSs) in West Africa. Brightness temperature data is required to identify and track MCSs. For this demonstrations, first 9 days of hrly data in August 2006 of the brightness temperature NCEP/CPC MERG dataset over location 5S - 40N, 60E - 90W will be used.\n\u003cbr\u003e\nMore information about the NCEP/CPC 4km MERG Dataset used for this demonstration can be found [here] (http://mirador.gsfc.nasa.gov/collections/MERG__001.shtml). \n\n\n\n\n\n##Step1: Load the SciSpark jar into Zeppelin\nThis jar contains all the classes and dependencies necessary to use SciSpark.\n\n\nIf you are receiving the following error: \n```\nMust be used before SparkInterpreter (%spark) initialized\nHint: put this paragraph before any Spark code and restart Zeppelin/Interpreter\n```\n**Restart the spark interpreter and go again!**\n",
      "dateUpdated": "Mar 3, 2016 12:52:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456441114561_-1813833419",
      "id": "20160225-145834_1785734285",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eMCS Algorithm to-date in SciSpark\u003c/h1\u003e\n\u003cp\u003ePurpose: This notebook will demonstrate using SciSpark to identify and track mesoscale convective systems (MCSs) in West Africa. Brightness temperature data is required to identify and track MCSs. For this demonstrations, first 9 days of hrly data in August 2006 of the brightness temperature NCEP/CPC MERG dataset over location 5S - 40N, 60E - 90W will be used.\n\u003cbr  /\u003e\u003cbr\u003e\n\u003cbr  /\u003eMore information about the NCEP/CPC 4km MERG Dataset used for this demonstration can be found \u003ca href\u003d\"http://mirador.gsfc.nasa.gov/collections/MERG__001.shtml\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003eStep1: Load the SciSpark jar into Zeppelin\u003c/h2\u003e\n\u003cp\u003eThis jar contains all the classes and dependencies necessary to use SciSpark.\u003c/p\u003e\n\u003cp\u003eIf you are receiving the following error:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eMust be used before SparkInterpreter (%spark) initialized\nHint: put this paragraph before any Spark code and restart Zeppelin/Interpreter\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eRestart the spark interpreter and go again!\u003c/strong\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2016 2:58:34 PM",
      "dateStarted": "Mar 3, 2016 12:52:57 PM",
      "dateFinished": "Mar 3, 2016 12:52:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%dep\n//z.load(\"/data/cluster-local/whitehal/SciSpark/target/scala-2.10/OrgDia.jar\")\nz.load(\"/data/cluster-local/whitehal/sujen_fork/SciSpark/target/scala-2.10/SciSparkTestExperiments.jar\")\n//z.load(\"/data/cluster-local/whitehal/SciSpark/target/scala-2.10/scisparktestexperiments_2.10-1.0.jar\")\n",
      "dateUpdated": "Mar 9, 2016 3:11:55 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "tableHide": false,
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1452192407462_-878085019",
      "id": "20160107-104647_252748255",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "res0: org.apache.zeppelin.spark.dep.Dependency \u003d org.apache.zeppelin.spark.dep.Dependency@73b1677e\n"
      },
      "dateCreated": "Jan 7, 2016 10:46:47 AM",
      "dateStarted": "Mar 9, 2016 3:11:55 PM",
      "dateFinished": "Mar 9, 2016 3:12:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//generic imports\nimport java.io.{ File, PrintWriter }\nimport java.text.SimpleDateFormat\nimport org.slf4j.Logger\nimport scala.collection.mutable\nimport scala.io.Source\nimport scala.language.implicitConversions\n//SciSpark imports\nimport org.dia.Parsers\nimport org.dia.core.{ SciSparkContext, SciTensor }\nimport org.dia.algorithms.mcc.MCCOps\nimport org.dia.algorithms.mcc.MCCNode\nimport org.dia.algorithms.mcc.MCCEdge",
      "dateUpdated": "Mar 9, 2016 3:14:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456437457261_-800414086",
      "id": "20160225-135737_1609073004",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.io.{File, PrintWriter}\nimport java.text.SimpleDateFormat\nimport org.slf4j.Logger\nimport scala.collection.mutable\nimport scala.io.Source\nimport scala.language.implicitConversions\nimport org.dia.Parsers\nimport org.dia.core.{SciSparkContext, SciTensor}\nimport org.dia.algorithms.mcc.MCCOps\nimport org.dia.algorithms.mcc.MCCNode\nimport org.dia.algorithms.mcc.MCCEdge\n"
      },
      "dateCreated": "Feb 25, 2016 1:57:37 PM",
      "dateStarted": "Mar 9, 2016 3:14:45 PM",
      "dateFinished": "Mar 9, 2016 3:15:10 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val args \u003d Array(\"mesos://scispark1.jpl.nasa.gov:5050\",\"6\",\"ch4\", \"hdfs://128.149.112.134:8090/data/48hrs\", \"/data/cluster-local/whitehal/SciSpark/merg_24_links.txt\")//MCCTestData\")\n//val args \u003d Array(\"local[2]\",\"6\",\"ch4\", \"hdfs://128.149.112.134:8090/data/48hrs\")//MCCTestData\")",
      "dateUpdated": "Mar 9, 2016 3:20:54 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456440457252_-970803130",
      "id": "20160225-144737_1873952919",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "args: Array[String] \u003d Array(mesos://scispark1.jpl.nasa.gov:5050, 6, ch4, hdfs://128.149.112.134:8090/data/48hrs, /data/cluster-local/whitehal/SciSpark/merg_24_links.txt)\n"
      },
      "dateCreated": "Feb 25, 2016 2:47:37 PM",
      "dateStarted": "Mar 9, 2016 3:20:54 PM",
      "dateFinished": "Mar 9, 2016 3:20:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// the user variables\nval masterURL \u003d if (args.length \u003c\u003d 1)  \"mesos://scispark1.jpl.nasa.gov:5050\" else args(0)\nval partCount \u003d if (args.length \u003c\u003d 2) 2 else args(1).toInt\nval varName \u003d if (args.length \u003c\u003d 3) \"ch4\" else args(2)\nval hdfspath \u003d if (args.length \u003c\u003d 4) args(3) else \"hdfs://128.149.112.134:8090/data/48hrs\" //July2006\" //else args(3)\nval inputFile \u003d if (args.length \u003c\u003d 5) args(4) else \"TRMM.txt\"\n\nval dateIndexTable \u003d new mutable.HashMap[String, Int]()\nval uris \u003d Source.fromFile(inputFile).mkString.split(\"\\n\").toList\n\nval orderedDateList \u003d uris.map(p \u003d\u003e {\n    p.split(\"/\").last.split(\"_\")(1) //replaceAllLiterally(\".\", \"/\")\n}).sorted\n\nfor (i \u003c- orderedDateList.indices) {\n    dateIndexTable +\u003d ((orderedDateList(i), i))\n}\n    \n// Initialize the spark context to point to the master URL\nval ssc \u003d new SciSparkContext(sc)",
      "dateUpdated": "Mar 9, 2016 3:21:02 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456440308446_1059349895",
      "id": "20160225-144508_467887620",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "masterURL: String \u003d mesos://scispark1.jpl.nasa.gov:5050\npartCount: Int \u003d 6\nvarName: String \u003d ch4\nhdfspath: String \u003d hdfs://128.149.112.134:8090/data/48hrs\ninputFile: String \u003d /data/cluster-local/whitehal/SciSpark/merg_24_links.txt\ndateIndexTable: scala.collection.mutable.HashMap[String,Int] \u003d Map()\nuris: List[String] \u003d List(/data/48hrs/merg_2006080100_4km-pixel.nc, /data/48hrs/merg_2006080101_4km-pixel.nc, /data/48hrs/merg_2006080102_4km-pixel.nc, /data/48hrs/merg_2006080103_4km-pixel.nc, /data/48hrs/merg_2006080104_4km-pixel.nc, /data/48hrs/merg_2006080105_4km-pixel.nc, /data/48hrs/merg_2006080106_4km-pixel.nc, /data/48hrs/merg_2006080107_4km-pixel.nc, /data/48hrs/merg_2006080108_4km-pixel.nc, /data/48hrs/merg_2006080109_4km-pixel.nc, /data/48hrs/merg_2006080110_4km-pixel.nc, /data/48hrs/merg_2006080111_4km-pixel.nc, /data/48hrs/merg_2006080112_4km-pixel.nc, /data/48hrs/merg_2006080113_4km-pixel.nc, /data/48hrs/merg_2006080114_4km-pixel.nc, /data/48hrs/merg_2006080115_4km-pixel.nc, /data/48hrs/merg_2006080116_4km-pixel.nc, /data/48hrs/merg_2006080117_4km-pixel.nc, /data/48hrs/mer...orderedDateList: List[String] \u003d List(2006080100, 2006080101, 2006080102, 2006080103, 2006080104, 2006080105, 2006080106, 2006080107, 2006080108, 2006080109, 2006080110, 2006080111, 2006080112, 2006080113, 2006080114, 2006080115, 2006080116, 2006080117, 2006080118, 2006080119, 2006080120, 2006080121, 2006080122, 2006080123)\nssc: org.dia.core.SciSparkContext \u003d org.dia.core.SciSparkContext@a831591\n"
      },
      "dateCreated": "Feb 25, 2016 2:45:08 PM",
      "dateStarted": "Mar 9, 2016 3:21:02 PM",
      "dateFinished": "Mar 9, 2016 3:21:05 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//val sRDD2 \u003d ssc.mergDFSFile(hdfspath, List(varName), partCount)\nval sRDD2 \u003d ssc.NetcdfDFSFile(hdfspath, List(varName), partCount)\n//val col2 \u003d sRDD2.collect()\nval labeled \u003d sRDD2.map(p \u003d\u003e {\n    val source \u003d p.metaData(\"SOURCE\").split(\"/\").last.split(\"_\")(1)\n    val FrameID \u003d source.toInt\n    p.insertDictionary((\"FRAME\", FrameID.toString))\n    p.insertDictionary((\"AREA\", \"0.0\"))\n    p\n})",
      "dateUpdated": "Mar 9, 2016 4:13:32 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456440877348_2057370452",
      "id": "20160225-145437_1814644650",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "sRDD2: org.apache.spark.rdd.RDD[org.dia.core.SciTensor] \u003d MapPartitionsRDD[44] at map at SciSparkContext.scala:104\nlabeled: org.apache.spark.rdd.RDD[org.dia.core.SciTensor] \u003d MapPartitionsRDD[45] at map at \u003cconsole\u003e:53\n"
      },
      "dateCreated": "Feb 25, 2016 2:54:37 PM",
      "dateStarted": "Mar 9, 2016 4:13:33 PM",
      "dateFinished": "Mar 9, 2016 4:13:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n##Step 6: Find cloud elements\nThe *cloud elements* are defined as contiguous areas that have a brightness temperature colder than 241K within a *frame*. As such a *frame* can have 0 or many *cloud elements*.\n\nNotice the **sciTensor** datatype that is accessed in the sRDD through the map and flatMap functions to mask data and add metadata tags (the *frame*).",
      "dateUpdated": "Mar 3, 2016 12:52:58 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456440906609_-1741624863",
      "id": "20160225-145506_995854616",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eStep 6: Find cloud elements\u003c/h2\u003e\n\u003cp\u003eThe \u003cem\u003ecloud elements\u003c/em\u003e are defined as contiguous areas that have a brightness temperature colder than 241K within a \u003cem\u003eframe\u003c/em\u003e. As such a \u003cem\u003eframe\u003c/em\u003e can have 0 or many \u003cem\u003ecloud elements\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eNotice the \u003cstrong\u003esciTensor\u003c/strong\u003e datatype that is accessed in the sRDD through the map and flatMap functions to mask data and add metadata tags (the \u003cem\u003eframe\u003c/em\u003e).\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2016 2:55:06 PM",
      "dateStarted": "Mar 3, 2016 12:52:59 PM",
      "dateFinished": "Mar 3, 2016 12:52:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val filtered \u003d labeled.map(p \u003d\u003e p(\"ch4\") \u003c\u003d 241.0)\n\nval consecFrames \u003d filtered.flatMap(p \u003d\u003e {\n    List((p.metaData(\"FRAME\").toInt, p, p.metaData(\"AREA\").toInt), (p.metaData(\"FRAME\").toInt + 1, p, p.metaData(\"AREA\").toInt))\n    // List((p.metaData(\"FRAME\").toInt, p), (p.metaData(\"FRAME\").toInt + 1, p))\n}).groupBy(_._1)\n    .map(p \u003d\u003e p._2.map(e \u003d\u003e e._2).toList)\n    .filter(p \u003d\u003e p.size \u003e 1)\n    .map(p \u003d\u003e p.sortBy(_.metaData(\"FRAME\").toInt))\n    .map(p \u003d\u003e (p(0), p(1)))\n",
      "dateUpdated": "Mar 9, 2016 4:13:36 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "tableHide": false,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456448890142_-493350305",
      "id": "20160225-170810_1878753352",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "filtered: org.apache.spark.rdd.RDD[org.dia.core.SciTensor] \u003d MapPartitionsRDD[46] at map at \u003cconsole\u003e:54\nconsecFrames: org.apache.spark.rdd.RDD[(org.dia.core.SciTensor, org.dia.core.SciTensor)] \u003d MapPartitionsRDD[53] at map at \u003cconsole\u003e:64\n"
      },
      "dateCreated": "Feb 25, 2016 5:08:10 PM",
      "dateStarted": "Mar 9, 2016 4:13:36 PM",
      "dateFinished": "Mar 9, 2016 4:13:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val componentFrameRDD \u003d consecFrames.flatMap({\n    case (t1, t2) \u003d\u003e {\n        val (components1, _) \u003d MCCOps.labelConnectedComponents(t1.tensor)\n        val (components2, _) \u003d MCCOps.labelConnectedComponents(t2.tensor)\n        val product \u003d components1 * components2\n        var overlappedPairsList \u003d mutable.MutableList[(Double, Double)]()\n        var areaMinMaxTable \u003d new mutable.HashMap[String, (Double, Double, Double)]\n\n    def updateComponent(label: Double, frame: String, value: Double): Unit \u003d {\n        if (label !\u003d 0.0) {\n            var area \u003d 0.0\n            var max \u003d Double.MaxValue\n            var min \u003d Double.MinValue\n            val currentProperties \u003d areaMinMaxTable.get(frame + \":\" + label)\n            if (currentProperties !\u003d null \u0026\u0026 currentProperties.isDefined) {\n              area \u003d currentProperties.get._1\n              max \u003d currentProperties.get._2\n              min \u003d currentProperties.get._3\n              if (value \u003c min) min \u003d value\n              if (value \u003e max) max \u003d value\n            } else {\n              min \u003d value\n              max \u003d value\n            }\n            area +\u003d 1\n            areaMinMaxTable +\u003d ((frame + \":\" + label, (area, max, min)))\n          }\n        }\n\n        for (row \u003c- 0 to product.rows - 1) {\n          for (col \u003c- 0 to product.cols - 1) {\n            if (product(row, col) !\u003d 0.0) {\n              val value1 \u003d components1(row, col)\n              val value2 \u003d components2(row, col)\n              overlappedPairsList +\u003d ((value1, value2))\n            }\n            updateComponent(components1(row, col), t1.metaData(\"FRAME\"), t1.tensor(row, col))\n            updateComponent(components2(row, col), t2.metaData(\"FRAME\"), t2.tensor(row, col))\n          }\n        }\n        \n    val edgesSet \u003d overlappedPairsList.toSet\n    val edges \u003d edgesSet.map({ case (c1, c2) \u003d\u003e ((t1.metaData(\"FRAME\"), c1), (t2.metaData(\"FRAME\"), c2)) })\n\n    val filtered \u003d edges.filter({\n          case ((frameId1, compId1), (frameId2, compId2)) \u003d\u003e {\n            val (area1, max1, min1) \u003d areaMinMaxTable(frameId1 + \":\" + compId1)\n            val isCloud1 \u003d ((area1 \u003e\u003d 2400.0) || ((area1 \u003c 2400.0) \u0026\u0026 ((min1/max1) \u003c 0.9)))\n            val (area2, max2, min2) \u003d areaMinMaxTable(frameId2 + \":\" + compId2)\n            val isCloud2 \u003d ((area2 \u003e\u003d 2400.0) || ((area2 \u003c 2400.0) \u0026\u0026 ((min2/max2) \u003c 0.9)))\n            isCloud1 \u0026\u0026 isCloud2\n          }\n        })\n        filtered\n      }\n    })",
      "dateUpdated": "Mar 9, 2016 4:13:46 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456449180507_-1107992989",
      "id": "20160225-171300_1026003931",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "componentFrameRDD: org.apache.spark.rdd.RDD[((String, Double), (String, Double))] \u003d MapPartitionsRDD[54] at flatMap at \u003cconsole\u003e:58\n"
      },
      "dateCreated": "Feb 25, 2016 5:13:00 PM",
      "dateStarted": "Mar 9, 2016 4:13:46 PM",
      "dateFinished": "Mar 9, 2016 4:13:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val collectedEdges \u003d componentFrameRDD.collect()\nval collectedVertices \u003d collectedEdges.flatMap({ case (n1, n2) \u003d\u003e List(n1, n2) }).toSet\n\nval outvertices \u003d new PrintWriter(new File(\"VertexList1.txt\"))\noutvertices.write(collectedVertices.toList.sortBy(_._1) + \"\\n\")\noutvertices.close()\n\nval outedges \u003d new PrintWriter(new File(\"EdgeList1.txt\"))\noutedges.write(collectedEdges.toList.sorted + \"\\n\")\noutedges.close()\n\nprintln(\"NUM VERTICES : \" + collectedVertices.size + \"\\n\")\nprintln(\"NUM EDGES : \" + collectedEdges.length + \"\\n\")\n",
      "dateUpdated": "Mar 9, 2016 3:40:01 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456511141312_120235175",
      "id": "20160226-102541_329920105",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "collectedEdges: Array[((String, Double), (String, Double))] \u003d Array(((2006080103,344.0),(2006080104,207.0)), ((2006080103,297.0),(2006080104,292.0)), ((2006080103,996.0),(2006080104,983.0)), ((2006080103,1342.0),(2006080104,1323.0)), ((2006080103,939.0),(2006080104,909.0)), ((2006080103,124.0),(2006080104,81.0)), ((2006080103,877.0),(2006080104,847.0)), ((2006080103,1073.0),(2006080104,1143.0)), ((2006080103,241.0),(2006080104,240.0)), ((2006080103,145.0),(2006080104,130.0)), ((2006080103,241.0),(2006080104,233.0)), ((2006080103,522.0),(2006080104,479.0)), ((2006080103,1064.0),(2006080104,1032.0)), ((2006080103,12.0),(2006080104,16.0)), ((2006080103,374.0),(2006080104,355.0)), ((2006080103,239.0),(2006080104,192.0)), ((2006080103,1030.0),(2006080104,999.0)), ((2006080103,1006.0),(200608...collectedVertices: scala.collection.immutable.Set[(String, Double)] \u003d Set((2006080121,603.0), (2006080123,1344.0), (2006080112,1008.0), (2006080121,1590.0), (2006080102,69.0), (2006080122,471.0), (2006080118,1044.0), (2006080105,1032.0), (2006080117,1383.0), (2006080110,270.0), (2006080113,743.0), (2006080104,768.0), (2006080117,1509.0), (2006080113,1382.0), (2006080119,1099.0), (2006080116,1128.0), (2006080101,1354.0), (2006080101,1061.0), (2006080114,1009.0), (2006080113,853.0), (2006080113,925.0), (2006080100,559.0), (2006080119,306.0), (2006080120,622.0), (2006080104,547.0), (2006080123,1113.0), (2006080116,1075.0), (2006080119,1299.0), (2006080101,633.0), (2006080123,576.0), (2006080112,1231.0), (2006080113,833.0), (2006080122,1197.0), (2006080113,1428.0), (2006080122,844.0), (2006...outvertices: java.io.PrintWriter \u003d java.io.PrintWriter@350c6986\noutedges: java.io.PrintWriter \u003d java.io.PrintWriter@7fceb000\nNUM VERTICES : 1773\n\nNUM EDGES : 1697\n\n"
      },
      "dateCreated": "Feb 26, 2016 10:25:41 AM",
      "dateStarted": "Mar 9, 2016 3:40:01 PM",
      "dateFinished": "Mar 9, 2016 3:40:09 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nAdding in finding the subgraphs",
      "dateUpdated": "Mar 9, 2016 3:41:04 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457566845615_-1390306276",
      "id": "20160309-154045_1007040181",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eAdding in finding the subgraphs\u003c/p\u003e\n"
      },
      "dateCreated": "Mar 9, 2016 3:40:45 PM",
      "dateStarted": "Mar 9, 2016 3:41:01 PM",
      "dateFinished": "Mar 9, 2016 3:41:01 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457566867144_2066693239",
      "id": "20160309-154107_415263906",
      "dateCreated": "Mar 9, 2016 3:41:07 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n## Step 9: Visualize ... coming soon\nWe will visualize the change of the cloud elements size over time\nDemonstrate \n* using Zeppelin context to transfer data from one environment to the next\n* use Python environment for data managling\n* use Sql environment for vis",
      "dateUpdated": "Mar 3, 2016 8:19:27 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/markdown",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1455155115192_2070241227",
      "id": "20160210-174515_97350177",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch2\u003eStep 9: Visualize \u0026hellip; coming soon\u003c/h2\u003e\n\u003cp\u003eWe will visualize the change of the cloud elements size over time\n\u003cbr  /\u003eDemonstrate\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusing Zeppelin context to transfer data from one environment to the next\u003c/li\u003e\n\u003cli\u003euse Python environment for data managling\u003c/li\u003e\n\u003cli\u003euse Sql environment for vis\u003c/li\u003e\n\u003c/ul\u003e\n"
      },
      "dateCreated": "Feb 10, 2016 5:45:15 PM",
      "dateStarted": "Mar 3, 2016 8:19:27 PM",
      "dateFinished": "Mar 3, 2016 8:19:27 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n##Step 8.1: Put data into Zeppelin context\nPut scala edgelist in Zeppelin context so it can be read by Python",
      "dateUpdated": "Mar 3, 2016 8:16:29 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1456512828625_1936090687",
      "id": "20160226-105348_1027919446",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT"
      },
      "dateCreated": "Feb 26, 2016 10:53:48 AM",
      "dateStarted": "Mar 3, 2016 12:53:32 PM",
      "dateFinished": "Mar 3, 2016 12:53:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import py4j.GatewayServer\nval gatewayServer \u003d new py4j.GatewayServer(null, 0)\ngatewayServer.start()\n//z.put(\"edgelist\",collectedEdges1)\nval vertices \u003d collectedVertices1.toList.sorted //.toArray\nz.put(\"cloudElements\",vertices)",
      "dateUpdated": "Mar 3, 2016 8:16:44 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457064981617_1044356877",
      "id": "20160303-201621_730738634",
      "dateCreated": "Mar 3, 2016 8:16:21 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n##Step 8.2: Get data from Zeppelin context\nRead scala edgelist from Zeppelin context into Python environment",
      "dateUpdated": "Mar 3, 2016 8:17:19 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457065022816_-1737331698",
      "id": "20160303-201702_1780335398",
      "dateCreated": "Mar 3, 2016 8:17:02 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nfrom py4j.java_gateway import JavaGateway\ngateway \u003d JavaGateway()\n\ncloudelements \u003d z.get(\"cloudElements\")\nprint type(cloudelements)\n#print len(cloudelements)\n#print type(cloudelements[0])",
      "dateUpdated": "Mar 3, 2016 8:17:00 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457064993569_2068589098",
      "id": "20160303-201633_1975927764",
      "dateCreated": "Mar 3, 2016 8:16:33 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n## Step 8.3: Get data in the csv format \nRight now there is sufficient data to generate a csv file with the format (time, CE). ",
      "dateUpdated": "Mar 3, 2016 8:17:33 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457065029366_-1623045951",
      "id": "20160303-201709_2076669066",
      "dateCreated": "Mar 3, 2016 8:17:09 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%pyspark\nimport pandas as pd\n\n#from py4j.java_gateway import JavaGateway\n#gateway \u003d JavaGateway()\nstack \u003d gateway.entry_point.getStack()\nnt_class \u003d gateway.jvm.int\n#l \u003d z.gateway.jvm.java.util.ArrayList()\n#print l\nallCEs \u003d []\n#for eachtuple in cloudelements:\n#    allCEs.append(eachtuple.replace(\"(\",\"\").replace(\")\",\"\")\n    ",
      "dateUpdated": "Mar 3, 2016 8:17:57 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1457065045494_-6918325",
      "id": "20160303-201725_1570778254",
      "dateCreated": "Mar 3, 2016 8:17:25 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Kim\u0027s notebook",
  "id": "2B8ZQ8FSH",
  "angularObjects": {
    "2BCC68R3T": [],
    "2B9AFX9BM": [],
    "2BBAYHPQT": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}